# Glossary of Terms & Acronyms

*   **AI (Artificial Intelligence):** Systems that can perform tasks that typically require human intelligence.
*   **LLMs (Large Language Models):** AI models trained on vast text corpora.
*   **Historical Bias (Societal Bias):** Prejudices and stereotypes present in historical texts, news articles, literature, and online content.
*   **Selection Bias (Sampling Bias):** Occurs when the data used for training or fine-tuning is not representative of the real-world distribution.
*   **Measurement Bias (Labeling Bias):** Inaccuracies or inconsistencies introduced during the labeling process for fine-tuning datasets.
*   **Algorithmic Bias (Model-Driven):** Bias introduced or amplified by the model's architecture, training objectives, or optimization algorithms.
*   **Optimization Bias:** If the optimization process prioritizes certain metrics (e.g., overall accuracy) without considering fairness metrics across subgroups, it can inadvertently lead to disparate performance.
*   **Prompt Bias (Human-Driven):** Bias introduced or amplified by the way a prompt is designed.
*   **Framing Bias:** The language used in the prompt can subtly steer the model towards a biased perspective.
*   **Example Bias (in Few-Shot):** If the examples provided in a few-shot prompt are not diverse or representative, the model might overfit to the biases present in those examples.
*   **Absence of Constraints:** Failing to explicitly instruct the model to be fair, neutral, or diverse can allow its inherent biases to manifest.
*   **Stereotyping:** Reinforcing harmful generalizations about groups.
*   **Harmful Generalizations:** Applying characteristics of a few individuals to an entire group.
*   **Underrepresentation/Exclusion:** Omitting or downplaying the contributions or existence of certain groups.
*   **Degradation/Disparagement:** Generating negative or demeaning content about specific groups.
*   **Performance Disparity:** The model performing worse (e.g., lower accuracy, higher error rates) for certain demographic groups or dialects.
*   **Fairness Metrics:** Quantitative metrics to assess if model performance is equitable across different subgroups.
*   **Bias Benchmarks:** Standardized datasets and tests designed to probe for specific biases.
*   **Human Audits:** Qualitative review by diverse groups of human evaluators to identify subtle biases.
*   **Red Teaming:** Proactively trying to elicit biased responses from the model.
*   **RAG (Retrieval-Augmented Generation):** Augmenting the LLM with information from curated, verified, and ideally debiased external knowledge bases.
