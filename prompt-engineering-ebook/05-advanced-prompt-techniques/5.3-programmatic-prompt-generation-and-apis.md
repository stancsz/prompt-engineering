# 5.3 Programmatic Prompt Generation & API Integration

While manual prompt engineering in playgrounds is excellent for experimentation, building real-world Large Language Model (LLM) applications requires **programmatic prompt generation** and robust **API integration**. This involves writing code to dynamically construct prompts, send them to LLM APIs, and process their responses. This approach enables automation, scalability, data-driven workflows, and seamless integration into existing software systems.

## Concept: Automating LLM Interactions

### 1. Prompt Templating Engines

**Role:** These tools allow you to define prompt structures with placeholders (variables) that are filled with dynamic content at runtime. This is a programmatic implementation of the "Static Templates and Dynamic Variables" pattern (Chapter 4.1).

*   **Mechanism:** You define a template string (e.g., `You are a {role}. Summarize this: {text}`). Your code then populates these placeholders with actual data before sending the complete prompt to the LLM.
*   **Benefits:** Ensures consistency, reusability, and maintainability of prompts across an application.
*   **Popular Libraries:**
    *   **Jinja2 (Python):** Widely used for templating in Python web frameworks and general text generation.
    *   **Mustache/Handlebars (JavaScript/various):** Cross-language templating systems.
    *   **F-strings (Python):** Simple string formatting for basic templating.

### 2. LLM SDKs and Libraries

**Role:** Software Development Kits (SDKs) provided by LLM providers (e.g., OpenAI Python SDK, Google Cloud Client Libraries) or higher-level frameworks (e.g., LangChain, Semantic Kernel) abstract away the complexities of direct API calls.

*   **Mechanism:** These SDKs provide convenient functions and classes to:
    *   Interact with LLM endpoints (e.g., `client.chat.completions.create()`).
    *   Manage prompt formatting (e.g., handling chat message roles like "system," "user," "assistant").
    *   Implement common patterns like prompt chaining, caching, and batching.
    *   Integrate with other components (e.g., vector databases for RAG).
*   **Benefits:** Simplifies development, reduces boilerplate code, and often includes built-in best practices for API interaction.

### 3. Direct API Integration

**Role:** Directly making HTTP requests to LLM endpoints. While SDKs are preferred, understanding direct API integration is useful for debugging, custom implementations, or when an SDK is not available.

*   **Mechanism:** Sending JSON payloads via HTTP POST requests to a specified API endpoint, including authentication headers (e.g., API keys).
*   **Benefits:** Maximum control over requests and responses.
*   **Considerations:** Requires manual handling of authentication, request formatting, response parsing, error handling, and retry logic.

## Benefits of Programmatic Prompting

*   **Automation:** Automate repetitive tasks (e.g., generating thousands of product descriptions).
*   **Scalability:** Handle high volumes of requests by integrating LLMs into scalable backend services.
*   **Dynamic Workflows:** Create prompts that adapt based on real-time data, user input, or application state.
*   **Integration:** Embed LLM capabilities directly into existing software applications, databases, and workflows.
*   **Version Control:** Manage prompts as code, allowing for versioning, collaboration, and CI/CD practices.
*   **Error Handling & Robustness:** Implement sophisticated error handling, retry mechanisms, and input/output validation.

## Key Considerations for Production Systems

*   **Authentication and Security:** **Never hardcode API keys directly in your code.** Securely manage API keys using environment variables, cloud secret management services (e.g., AWS Secrets Manager, Azure Key Vault, Google Secret Manager), or dedicated configuration files that are not committed to version control.
*   **Rate Limits:** LLM APIs often have rate limits (requests per minute/second). Implement exponential backoff and retry logic to handle these gracefully, preventing your application from crashing or being throttled.
*   **Cost Management:** Monitor token usage and costs closely. Optimize prompts for conciseness, choose appropriate models (smaller models for simpler tasks), and consider caching strategies to reduce redundant LLM calls.
*   **Latency:** Minimize API call latency through efficient prompt design, batching multiple requests where possible, and using asynchronous API calls in high-throughput applications.
*   **Error Handling:** Anticipate and handle various API errors (e.g., invalid requests, authentication failures, server errors, rate limits). Implement robust `try-except` blocks and logging.
*   **Input/Output Validation:** Validate inputs before sending to the LLM to prevent unexpected behavior or prompt injection attacks. Parse and validate LLM outputs to ensure they meet expected formats (e.g., JSON schema validation) and content requirements.
*   **Observability:** Implement comprehensive logging, monitoring, and tracing to understand LLM performance, track token usage, identify bottlenecks, and debug issues in production environments.
*   **Version Control:** Treat your prompts and prompt templates as code. Store them in version control systems (e.g., Git) to track changes, enable collaboration, and facilitate CI/CD practices.

## Example: Dynamic Content Generation with Python & OpenAI API

This example demonstrates generating personalized email subject lines and body content using Jinja2 templating and the OpenAI Python SDK, including basic retry logic.

**Setup:**
1.  **Python Environment:** Ensure you have Python 3.8+ installed.
2.  **Install Libraries:**
    ```bash
    pip install openai jinja2
    ```
3.  **OpenAI API Key:** Set your OpenAI API key as an environment variable named `OPENAI_API_KEY`.
    *   **Windows (Command Prompt):** `set OPENAI_API_KEY=your_api_key_here`
    *   **Linux/macOS (Bash/Zsh):** `export OPENAI_API_KEY=your_api_key_here`

**`dynamic_email_generator.py`:**

```python
import os
import time
import json
from jinja2 import Template
from openai import OpenAI
from openai import RateLimitError, APIError

# Initialize OpenAI client
try:
    client = OpenAI()
except Exception as e:
    print(f"Error initializing OpenAI client. Ensure OPENAI_API_KEY is set: {e}")
    exit()

# 1. Define the Prompt Templates (more complex Jinja2 example)
email_template = Template("""
You are a marketing copywriter.
Generate an email for a product launch based on the following details.

Product Name: {{ product_name }}
Key Features:
{% for feature in key_features %}
- {{ feature }}
{% endfor %}
Target Audience: {{ target_audience }}
Tone: {{ tone }}
Call to Action: {{ call_to_action }}

{% if special_offer %}
Special Offer: {{ special_offer }}
{% endif %}

Email Subject Line (under 70 characters):
Email Body:
""")

# 2. Prepare Dynamic Data
product_data = [
    {
        "product_name": "QuantumFlow Smartwatch",
        "key_features": ["All-day battery life", "Heart rate monitoring", "GPS tracking"],
        "target_audience": "Fitness enthusiasts and tech-savvy individuals",
        "tone": "exciting and innovative",
        "call_to_action": "Pre-order now!",
        "special_offer": "First 100 customers get 20% off!"
    },
    {
        "product_name": "EcoClean Laundry Pods",
        "key_features": ["Plant-based ingredients", "Zero waste packaging", "Effective stain removal"],
        "target_audience": "Eco-conscious families",
        "tone": "informative and trustworthy",
        "call_to_action": "Learn more and subscribe",
        "special_offer": None # No special offer for this product
    }
]

# Function to call LLM with retry logic
def get_llm_response_with_retry(prompt_content, model="gpt-3.5-turbo", temperature=0.7, max_tokens=300, retries=3, delay=1):
    for i in range(retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt_content}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content.strip()
        except RateLimitError:
            print(f"Rate limit hit. Retrying in {delay} seconds...")
            time.sleep(delay)
            delay *= 2 # Exponential backoff
        except APIError as e:
            print(f"API Error: {e}. Retrying...")
            time.sleep(delay)
            delay *= 2
        except Exception as e:
            print(f"An unexpected error occurred: {e}")
            break # Exit on other errors

    print(f"Failed to get LLM response after {retries} retries.")
    return None

# 3. Iterate and Generate Prompts & Call API
generated_emails = []

for data in product_data:
    # Render the prompt with dynamic data
    prompt_content = email_template.render(
        product_name=data["product_name"],
        key_features=data["key_features"],
        target_audience=data["target_audience"],
        tone=data["tone"],
        call_to_action=data["call_to_action"],
        special_offer=data["special_offer"]
    )

    print(f"\n--- Generating email for {data['product_name']} ---")
    # print(f"Prompt:\n{prompt_content}") # Uncomment to see the full prompt

    email_output = get_llm_response_with_retry(prompt_content)

    if email_output:
        # Basic parsing (you might use regex or more robust parsing for production)
        subject_line = "N/A"
        email_body = "N/A"
        lines = email_output.split('\n')
        if "Email Subject Line:" in lines[0]:
            subject_line = lines[0].replace("Email Subject Line:", "").strip()
            email_body = "\n".join(lines[1:]).strip()
        elif "Subject Line:" in lines[0]: # Handle slight variations
            subject_line = lines[0].replace("Subject Line:", "").strip()
            email_body = "\n".join(lines[1:]).strip()
        else:
            # Fallback if parsing fails, assume first line is subject, rest is body
            subject_line = lines[0].strip()
            email_body = "\n".join(lines[1:]).strip()


        generated_emails.append({
            "product": data["product_name"],
            "subject_line": subject_line,
            "email_body": email_body
        })
        print(f"Generated Subject: {subject_line}")
        # print(f"Generated Body:\n{email_body}") # Uncomment to see the full body
    else:
        generated_emails.append({
            "product": data["product_name"],
            "subject_line": "Generation Failed",
            "email_body": "N/A"
        })

print("\n--- All Generated Emails ---")
print(json.dumps(generated_emails, indent=2))
```

**To Run This Exercise:**
1.  Save the code above as `dynamic_email_generator.py`.
2.  Open your terminal in the same directory.
3.  Run: `python dynamic_email_generator.py`

## Hands-On Exercise: Building a Programmatic Summarizer

This exercise guides you through creating a Python script to summarize multiple articles programmatically using the OpenAI API.

**Setup:**
1.  **Python Environment:** Ensure you have Python 3.8+ installed.
2.  **Install Libraries:**
    ```bash
    pip install openai jinja2 pandas
    ```
3.  **OpenAI API Key:** Set your OpenAI API key as an environment variable named `OPENAI_API_KEY`. (Refer to the "Key Considerations for Production Systems" section for secure management practices.)

**Steps:**
1.  **Create the Script:** A complete Python script named `summarizer.py` has been provided in this chapter's directory (`chapters/05-advanced-prompt-techniques/summarizer.py`). This script:
    *   Reads articles from a `articles.csv` file (it will create a dummy one if not found).
    *   Uses a Jinja2 template to construct summarization prompts.
    *   Calls the OpenAI API with retry logic.
    *   Saves the generated summaries to `summaries.csv`.
2.  **Review the Code:** Open `chapters/05-advanced-prompt-techniques/summarizer.py` and examine its contents. Pay attention to:
    *   The `summarization_template` using Jinja2.
    *   The `get_llm_summary_with_retry` function for robust API calls.
    *   The `main` function's logic for reading input, processing, and saving output.
3.  **Run and Verify:**
    *   Open your terminal in the `chapters/05-advanced-prompt-techniques/` directory.
    *   Run the script: `python summarizer.py`
    *   Observe the terminal output, which will show the processing of each article and the final `summaries.csv` content.
    *   Check for the newly created `articles.csv` (if it didn't exist) and `summaries.csv` files in the same directory.

**`articles.csv` (Example Content - created automatically if not present):**
```csv
id,text
1,"The recent discovery of a new exoplanet, Kepler-186f, has excited astronomers. It is the first Earth-size planet found in the habitable zone of another star, suggesting it could potentially harbor liquid water and life. This finding opens new avenues for the search for extraterrestrial life."
2,"Quantum computing is a rapidly emerging technology that harnesses the principles of quantum mechanics to solve problems too complex for classical computers. Unlike classical bits, which are either 0 or 1, quantum bits (qubits) can be both simultaneously, enabling exponential processing power."
3,"Artificial intelligence (AI) is rapidly transforming various industries, from healthcare to finance. Machine learning, a subset of AI, enables systems to learn from data without explicit programming. Deep learning, a further subset, uses neural networks with many layers to achieve state-of-the-art performance in tasks like image recognition and natural language processing."
```

## Reflection

*   How did programmatic generation improve the efficiency of processing multiple articles compared to manual prompting?
*   What challenges did you face in parsing the LLM's output or handling potential API errors?
*   How would you extend this script to include more advanced features like:
    *   Batching multiple articles into a single API call (if supported by the model).
    *   Implementing exponential backoff for rate limit handling.
    *   Adding a mechanism to store and retrieve prompts from a database instead of hardcoding them?
*   Consider the security implications of handling API keys in a production environment.

## Best Practices for Programmatic LLM Integration

*   **Modularize Prompts:** Store prompt templates separately from your application logic (e.g., in `.txt` or `.jinja` files).
*   **Use SDKs:** Leverage official LLM SDKs for ease of use, built-in features, and adherence to API best practices.
*   **Structured Outputs:** Whenever possible, instruct the LLM to generate structured outputs (JSON, XML) and use robust parsing libraries to process them.
*   **Error Handling & Retries:** Implement comprehensive error handling, including retry mechanisms with exponential backoff for transient network or rate limit errors.
*   **Asynchronous Calls:** For high-throughput applications, use asynchronous API calls to prevent blocking.
*   **Input Sanitization:** Sanitize user inputs before injecting them into prompts to prevent prompt injection attacks.
*   **Output Validation:** Validate the LLM's output against expected schemas or rules to ensure correctness and safety.
*   **Logging and Monitoring:** Log API requests, responses, and errors. Monitor token usage and latency in production.
*   **Cost Optimization:** Be mindful of token usage. Experiment with different models (smaller models for simpler tasks) and optimize prompt length.
*   **Version Control:** Treat your prompts and prompt templates as code and manage them in version control.
