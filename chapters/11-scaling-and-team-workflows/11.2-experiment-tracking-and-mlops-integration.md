# 11.2 Experiment Tracking & MLOps Integration

Systematic tracking of prompts, configurations, and results enables reproducibility and collaboration.

## Key Concepts

- **Run Tracking:** Record prompt text, parameters (temperature, examples), and outcomes.  
- **Metadata:** Tags for prompt version, dataset, model, and user.  
- **Visualization:** Dashboards for comparing runs over time.  
- **Integration:** Connect prompt experiments into an MLOps pipeline for CI, monitoring, and deployment.

## Tool Examples

- **Weights & Biases (wandb):**  
  ```python
  import wandb
  wandb.init(project="prompt-engineering", name="summarization-v1")
  wandb.config.update({"temperature":0.3, "model":"gpt-4"})
  run = wandb.log({"response": response_text})
  ```
- **MLflow:**  
  ```python
  import mlflow
  with mlflow.start_run(run_name="classification-test"):
      mlflow.log_params({"top_k":50, "top_p":0.95})
      mlflow.log_metric("accuracy", accuracy_score)
  ```
- **ZenML / Kedro:** Pipelines orchestrated end-to-end from data to evaluation.

## Hands-On Exercise

1. Choose a prompt variant and a metric (e.g., BLEU score).  
2. Set up W&B or MLflow and log:
   - Prompt version  
   - Generation parameters  
   - Evaluation metric  
3. Run 5â€“10 experiments varying a single parameter.  
4. Use the web UI to compare runs and identify best configuration.

## Reflection

- Which parameter had the biggest impact on performance?  
- How did tracking help detect regressions?  
- What integration steps would you automate in a CI/CD pipeline?
