# 11.2 Experiment Tracking & MLOps Integration for Prompts

Just as machine learning models require rigorous **experiment tracking** and **MLOps (Machine Learning Operations)** practices, so too do Large Language Model (LLM) prompts. Prompt engineering is an iterative, experimental process, and without systematic tracking of prompt versions, configurations, inputs, and outputs, it's impossible to reproduce results, compare performance objectively, collaborate effectively, or ensure reliable deployments. This chapter explores how to integrate prompt experimentation into an MLOps framework.

## Key Concepts

### 1. Experiment Run Tracking

**Definition:** Recording all relevant information about each "run" or iteration of a prompt experiment. A run represents a single execution of a prompt with specific inputs and parameters, yielding a particular output.

*   **What to Track per Run:**
    *   **Prompt Text/ID/Version:** The exact prompt template used (linked to a prompt library, Chapter 11.1).
    *   **Input Data:** The specific user query or input text.
    *   **LLM Parameters:** Decoding controls (temperature, top-p, max_tokens), model name/version.
    *   **Generated Output:** The full response from the LLM.
    *   **Evaluation Metrics:** Automated metrics (BLEU, ROUGE, Embedding Similarity, accuracy) and/or links to human evaluation results (Chapter 6).
    *   **Metadata:** Timestamp, user/experimenter ID, unique run ID, tags (e.g., "A/B test variant X", "bug fix").
    *   **Intermediate Steps:** For prompt chains or agents, log the inputs/outputs of each sub-step (tracing).

### 2. Metadata and Tagging

**Definition:** Attaching descriptive information to runs and prompts to facilitate search, filtering, and analysis.

*   **Purpose:** Enables quick identification of successful experiments, problematic prompts, or specific configurations.
*   **Examples:** Tags like `summarization`, `customer-support`, `gpt-4`, `low-temp`, `production-candidate`.

### 3. Visualization and Dashboards

**Definition:** Using interactive dashboards to compare experiment runs, visualize performance trends, and identify optimal prompt configurations.

*   **Benefits:**
    *   **Comparative Analysis:** Side-by-side comparison of prompt outputs and metrics.
    *   **Trend Monitoring:** Track how prompt performance evolves over time or with different parameter settings.
    *   **Hyperparameter Tuning:** Visualize the impact of decoding controls or prompt variations on metrics.
    *   **Debugging:** Quickly identify runs that led to errors or undesirable outputs.

### 4. MLOps Integration

**Definition:** Connecting prompt experimentation and management into the broader Machine Learning Operations (MLOps) lifecycle, which encompasses data management, model development, deployment, and monitoring.

*   **Key Integration Points:**
    *   **Data Versioning:** Link prompt experiments to the specific input data versions used.
    *   **Model Registry:** Track which LLM model version was used for each prompt run.
    *   **CI/CD for Prompts:** Automate testing and deployment of new prompt versions (Chapter 11.4).
    *   **Monitoring:** Use experiment tracking data to feed production monitoring dashboards (Chapter 7.3).
    *   **Feedback Loops:** Integrate user feedback and production data back into the experiment tracking system for continuous improvement.

## Tool Examples for Experiment Tracking

### 1. Weights & Biases (W&B)

*   **Role:** A popular MLOps platform for experiment tracking, visualization, and collaboration. W&B Prompts is a specific feature for LLM observability.
*   **Features:** Log prompts, responses, metrics, media (e.g., generated images), and system metadata. Create interactive dashboards to compare runs.
*   **Example:**
    ```python
    import wandb
    from openai import OpenAI
    import os

    # wandb.login() # You might need to run this once to log in

    client = OpenAI()

    def run_and_log_prompt(prompt_text, temp, model_name, run_name):
        with wandb.init(project="llm-prompt-experiments", name=run_name, reinit=True) as run:
            run.config.update({
                "prompt_text": prompt_text,
                "temperature": temp,
                "model": model_name
            })

            try:
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": prompt_text}],
                    temperature=temp,
                    max_tokens=100
                )
                output = response.choices[0].message.content.strip()
                input_tokens = response.usage.prompt_tokens
                output_tokens = response.usage.completion_tokens

                run.log({
                    "generated_output": output,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": input_tokens + output_tokens,
                    "success": True
                })
                print(f"Run '{run_name}' logged successfully. Output: {output[:50]}...")
            except Exception as e:
                run.log({
                    "error": str(e),
                    "success": False
                })
                print(f"Run '{run_name}' failed: {e}")

    # Example experiments
    run_and_log_prompt("Write a short, creative story.", 0.7, "gpt-3.5-turbo", "creative-story-temp0.7")
    run_and_log_prompt("Write a short, creative story.", 1.0, "gpt-3.5-turbo", "creative-story-temp1.0")
    run_and_log_prompt("Summarize the theory of relativity in 3 sentences.", 0.2, "gpt-3.5-turbo", "summarization-temp0.2")
    ```

### 2. MLflow

*   **Role:** An open-source platform for managing the end-to-end machine learning lifecycle, including experiment tracking, reproducible runs, and model management.
*   **Features:** Log parameters, metrics, artifacts (e.g., prompt files, generated outputs), and source code. Provides a UI for comparing runs.
*   **Example:**
    ```python
    import mlflow
    from openai import OpenAI
    import os

    # Set MLflow tracking URI (e.g., to a local folder or remote server)
    # mlflow.set_tracking_uri("file:///tmp/mlruns")

    client = OpenAI()

    def run_and_log_prompt_mlflow(prompt_text, temp, model_name, run_name):
        with mlflow.start_run(run_name=run_name) as run:
            mlflow.log_param("prompt_text", prompt_text)
            mlflow.log_param("temperature", temp)
            mlflow.log_param("model", model_name)

            try:
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": prompt_text}],
                    temperature=temp,
                    max_tokens=100
                )
                output = response.choices[0].message.content.strip()
                input_tokens = response.usage.prompt_tokens
                output_tokens = response.usage.completion_tokens

                mlflow.log_metric("input_tokens", input_tokens)
                mlflow.log_metric("output_tokens", output_tokens)
                mlflow.log_metric("total_tokens", input_tokens + output_tokens)
                mlflow.log_metric("success", 1)
                mlflow.log_text(output, "generated_output.txt") # Log output as an artifact
                print(f"Run '{run_name}' logged successfully. Output: {output[:50]}...")
            except Exception as e:
                mlflow.log_metric("success", 0)
                mlflow.log_text(str(e), "error_message.txt")
                print(f"Run '{run_name}' failed: {e}")

    # Example experiments
    run_and_log_prompt_mlflow("Write a short, creative story.", 0.7, "gpt-3.5-turbo", "creative-story-mlflow-temp0.7")
    run_and_log_prompt_mlflow("Write a short, creative story.", 1.0, "gpt-3.5-turbo", "creative-story-mlflow-temp1.0")
    ```

### 3. LangSmith

*   **Role:** A dedicated platform by LangChain for debugging, testing, evaluating, and monitoring LLM applications built with LangChain.
*   **Features:** Detailed traces of LLM calls (including intermediate steps in chains/agents), dataset management for evaluation, prompt versioning, and performance dashboards.
*   **Integration:** Seamlessly integrates with LangChain applications.

### 4. ZenML / Kedro (Orchestration Frameworks)

*   **Role:** These are MLOps frameworks that help build and orchestrate end-to-end machine learning pipelines. They can be used to define pipelines where prompt generation, LLM calls, and evaluation are distinct steps.
*   **Benefits:** Enforce reproducibility, manage data dependencies, and automate complex workflows.

## Hands-On Exercise: Tracking Prompt Experiments

*Note: This exercise requires an account with Weights & Biases (wandb.ai) and setting your `WANDB_API_KEY` environment variable.*

1.  **Setup:**
    *   Install `wandb` and `openai`: `pip install wandb openai`
    *   Run `wandb login` in your terminal and follow the instructions.
2.  **Create an Experiment Script:**
    *   Create a Python file (e.g., `prompt_experiment.py`).
    *   Define a function that takes a prompt, temperature, and model, calls the LLM, and logs the results to W&B.
    *   Run a few experiments with different temperatures or prompt variations.

    ```python
    import wandb
    from openai import OpenAI
    import os
    import time

    # Ensure OPENAI_API_KEY and WANDB_API_KEY are set as environment variables

    client = OpenAI()

    def run_llm_experiment(prompt_id, prompt_text, model_name, temperature, max_tokens=100):
        # Initialize a W&B run
        with wandb.init(project="my-llm-prompts", job_type="prompt_test", name=f"{prompt_id}-temp-{temperature}", reinit=True) as run:
            # Log configuration parameters
            run.config.update({
                "prompt_id": prompt_id,
                "model": model_name,
                "temperature": temperature,
                "max_tokens": max_tokens
            })

            # Log the full prompt text as an artifact or config
            run.log({"prompt_content": prompt_text})

            start_time = time.time()
            try:
                # Call the LLM
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[{"role": "user", "content": prompt_text}],
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                end_time = time.time()
                
                output_text = response.choices[0].message.content.strip()
                latency = (end_time - start_time) * 1000 # ms
                input_tokens = response.usage.prompt_tokens
                output_tokens = response.usage.completion_tokens

                # Log metrics and output
                run.log({
                    "generated_output": output_text,
                    "latency_ms": latency,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": input_tokens + output_tokens,
                    "success": True
                })
                print(f"Logged run for {prompt_id} (temp={temperature}). Output: {output_text[:50]}...")

            except Exception as e:
                end_time = time.time()
                latency = (end_time - start_time) * 1000
                run.log({
                    "error_message": str(e),
                    "latency_ms": latency,
                    "success": False
                })
                print(f"Error in run for {prompt_id} (temp={temperature}): {e}")

    # --- Run your experiments ---
    base_prompt = "Write a short, creative story about a robot who learns to paint."
    model = "gpt-3.5-turbo"

    # Experiment 1: Vary temperature
    run_llm_experiment("robot_story_creative", base_prompt, model, 0.7)
    run_llm_experiment("robot_story_creative", base_prompt, model, 1.0)
    run_llm_experiment("robot_story_creative", base_prompt, model, 1.2)

    # Experiment 2: Vary prompt instruction
    prompt_variant_1 = "Write a concise, factual summary of the history of AI."
    prompt_variant_2 = "You are a historian. Provide a brief, engaging overview of the history of AI, focusing on key milestones."
    run_llm_experiment("ai_history_factual", prompt_variant_1, model, 0.3)
    run_llm_experiment("ai_history_engaging", prompt_variant_2, model, 0.7)
    ```
3.  **Run the Script:** Execute `python prompt_experiment.py`.
4.  **Explore W&B UI:** Go to your Weights & Biases project dashboard (link will be printed in your terminal). Explore the runs, compare their configurations, outputs, and metrics. Create custom charts to visualize the impact of temperature on output length or creativity (if you had a human evaluation metric).

## Reflection

*   How did the experiment tracking platform help you organize and compare different prompt variations?
*   Which logged metadata (e.g., temperature, prompt text, token count) was most useful for understanding the results of your experiments?
*   How would you use the W&B UI to identify the "best" prompt for a specific goal (e.g., highest creativity, lowest token usage)?
*   What are the benefits of integrating prompt tracking into a dedicated MLOps platform versus just using local logs or notebooks?

## Challenges and Best Practices

### Challenges:

*   **Data Volume:** Logging every prompt and response can generate a lot of data, requiring robust storage solutions.
*   **Inconsistent Logging:** Ensuring all relevant information is logged consistently across different parts of an application.
*   **Parsing Outputs:** For complex generative tasks, automatically extracting metrics from free-form text outputs can be challenging.
*   **Human Evaluation Integration:** Linking automated metrics with subjective human feedback in a unified tracking system.
*   **Cost:** Some MLOps platforms have usage-based pricing.

### Best Practices:

*   **Define Logging Schema:** Standardize what information is logged for each LLM interaction.
*   **Automate Logging:** Integrate logging directly into your LLM wrapper functions or SDK calls.
*   **Leverage MLOps Platforms:** Use tools like W&B, MLflow, or LangSmith for comprehensive tracking and visualization.
*   **Tag Everything:** Use tags to categorize runs by prompt version, feature, experiment type, etc.
*   **Link to Prompt Library:** Ensure tracked runs reference the specific version of the prompt from your prompt library.
*   **Visualize Key Metrics:** Create dashboards that highlight critical performance indicators and allow for easy comparison.
*   **Continuous Evaluation:** Integrate experiment tracking with automated evaluation pipelines (Chapter 11.4) to continuously monitor prompt performance.
*   **Security and Privacy:** Be mindful of sensitive data in logs and tracking systems.
