# 5.4 Parameter-Efficient Fine-Tuning (PEFT): Prefix Tuning, Adapters, and Soft Prompts

Large Language Models (LLMs) are incredibly powerful, but fully fine-tuning them for every new task is computationally expensive, requires large datasets, and creates a new, large model checkpoint for each task. **Parameter-Efficient Fine-Tuning (PEFT)** methods offer a lightweight alternative, allowing you to adapt LLMs to specific tasks or domains by training only a small fraction of the model's parameters. This chapter explores key PEFT techniques like Prefix Tuning, Adapters, and Soft Prompts, and their relevance to prompt engineering.

## Why PEFT Methods Matter

*   **Reduced Computational Cost:** Train only a small number of parameters, significantly lowering GPU memory and compute requirements.
*   **Faster Training:** Training converges much quicker than full fine-tuning.
*   **Storage Efficiency:** Store only small, task-specific "adapter" modules or "soft prompts" instead of entire model copies.
*   **Modularity:** Easily swap out task-specific modules without reloading the entire base model.
*   **Avoid Catastrophic Forgetting:** The frozen base model retains its general knowledge, preventing degradation on other tasks.

## Key PEFT Techniques

### 1. Prefix Tuning

**Definition:** Prefix tuning involves learning a small, continuous sequence of task-specific vectors (the "prefix") that are prepended to the input embeddings at each layer of the Transformer model. The base LLM's parameters remain frozen; only these prefix vectors are trained.

*   **Mechanism:** Instead of adding discrete, human-readable tokens to the prompt, prefix tuning adds a learned, continuous "virtual prompt" in the model's embedding space. This prefix guides the model's internal representations to steer its behavior towards the desired task.
*   **Analogy:** Imagine you're giving the model a "mental nudge" or a "pre-computation" at every layer, rather than just at the input.
*   **Benefits:** Highly parameter-efficient (often training <0.1% of total parameters), effective for various NLP tasks.
*   **Limitations:** The learned prefix is not human-interpretable. Requires a training dataset for each task.

**Example Workflow (Conceptual Training & Inference):**

1.  **Initialization:** Initialize a small, trainable matrix of vectors (the "prefix") for each Transformer layer.
2.  **Training:** For a specific task (e.g., sentiment classification), feed labeled data through the LLM. During backpropagation, only the parameters of the prefix vectors are updated, while the main LLM weights are frozen.
3.  **Inference:** When making a prediction, the learned prefix vectors are prepended to the input embeddings of the prompt at each layer, guiding the LLM to produce the desired output for that task.

### 2. Adapters (Adapter-based Fine-Tuning)

**Definition:** Adapters are small, lightweight neural network modules inserted *within* each layer of a pre-trained Transformer model. During fine-tuning, only the parameters of these adapter modules are trained, while the original LLM weights remain frozen.

*   **Mechanism:** An adapter typically consists of a down-projection, a non-linear activation, and an up-projection, creating a bottleneck. This bottleneck allows the adapter to learn task-specific transformations with very few parameters.
*   **Benefits:**
    *   **Modularity:** Different tasks can have their own adapter modules, which can be easily swapped in and out.
    *   **Composability:** Multiple adapters can be stacked or combined for multi-task learning or sequential task execution.
    *   **Parameter Efficiency:** Similar to prefix tuning, only a tiny fraction of parameters are trained.
*   **Limitations:** Can add a slight increase in inference latency due to the additional layers.

**Example Hands-On (using `huggingface/adapters` library):**

```python
# This is a conceptual example. Full setup requires dataset loading and training loop.
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdapterConfig
from datasets import load_dataset

# 1. Load a pre-trained model and tokenizer
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# 2. Define an Adapter configuration
# This creates a bottleneck adapter with a reduction factor of 16
adapter_config = AdapterConfig.load("pfeiffer", reduction_factor=16)

# 3. Add a new adapter for a specific task (e.g., sentiment analysis)
model.add_adapter("sentiment_analysis", config=adapter_config)

# 4. Set the adapter to be active for training
model.train_adapter("sentiment_analysis")

# 5. (Conceptual) Train the model on a sentiment dataset
# This would involve preparing a dataset, defining a training loop,
# and calling model.train() or using a Trainer from Hugging Face.
# During training, only the adapter parameters are updated.

# 6. (Conceptual) Save and Load the adapter
# model.save_adapter("./my_sentiment_adapter", "sentiment_analysis")
# model.load_adapter("./my_sentiment_adapter", "sentiment_analysis")

# 7. (Conceptual) Activate the adapter for inference
# model.set_active_adapter("sentiment_analysis")
# Then, use the model for sentiment prediction.
```

### 3. Soft Prompts (Prompt Tuning / Prompt Learning)

**Definition:** Soft prompts (or prompt tuning/prompt learning) is a broader category of PEFT methods where continuous, trainable vectors are learned and prepended to the input sequence, similar to prefix tuning. The key distinction is that these "prompts" are not human-readable tokens but rather optimized numerical embeddings.

*   **Mechanism:** Instead of manually crafting discrete text prompts, you define a fixed-length sequence of "virtual tokens" whose embeddings are randomly initialized and then optimized through backpropagation on a downstream task. These learned embeddings are then prepended to the input embeddings of the actual text.
*   **Benefits:** Extremely parameter-efficient, can achieve performance comparable to full fine-tuning on many tasks, and are highly flexible.
*   **Limitations:** The learned soft prompt is not interpretable by humans. Requires a training dataset.

**Example Workflow (Conceptual Training & Inference):**

1.  **Define Soft Prompt Length:** Choose a length for your soft prompt (e.g., 10-100 virtual tokens).
2.  **Initialize:** Create a trainable tensor of embeddings for these virtual tokens.
3.  **Training:** During training, these soft prompt embeddings are concatenated with the embeddings of your input text. Only the soft prompt embeddings are updated, while the main LLM weights are frozen.
4.  **Inference:** The learned soft prompt embeddings are prepended to the input embeddings of new queries, guiding the LLM's response.

## Comparison and When to Use Which

| Feature             | Full Fine-Tuning                               | Prefix Tuning / Soft Prompts                     | Adapters                                         |
| :------------------ | :--------------------------------------------- | :----------------------------------------------- | :----------------------------------------------- |
| **Parameters Trained** | All LLM parameters                             | ~0.1% of LLM parameters (prefix/soft prompt)     | ~0.1-5% of LLM parameters (adapter modules)      |
| **Storage Cost**    | High (new full model checkpoint per task)      | Low (small prefix/adapter weights per task)      | Low (small adapter weights per task)             |
| **Training Speed**  | Slow                                           | Fast                                             | Fast                                             |
| **Inference Latency** | Base model latency                             | Base model latency (minimal overhead)            | Slightly increased (due to adapter layers)       |
| **Interpretability**| N/A                                            | Low (learned vectors)                            | Low (learned weights in bottleneck layers)       |
| **Modularity**      | Low (new model per task)                       | High (small prefixes can be swapped)             | Very High (adapters can be stacked/swapped)      |
| **Best For**        | Significant domain adaptation, high performance on specific, complex tasks | Many diverse tasks, resource-constrained environments | Multi-task learning, composable AI systems       |

**When to Choose:**

*   **Full Fine-Tuning:** When you need the absolute highest performance for a critical task, have a large, high-quality labeled dataset, and can afford the computational resources and storage.
*   **Prefix Tuning / Soft Prompts:** When you have many tasks and want to adapt a single base LLM to all of them efficiently, or when you are resource-constrained.
*   **Adapters:** Similar to prefix tuning, but offers more modularity and composability, making it ideal for scenarios where you might want to combine multiple task-specific behaviors.
*   **Prompt Engineering (Discrete Prompts):** For rapid prototyping, tasks that don't require extreme specialization, or when you don't have labeled data for training. This is the most flexible and immediate approach.

## Hands-On Exercise: Conceptual Prompt Tuning

*Note: Implementing prompt tuning from scratch is complex. This exercise focuses on understanding the concept using a high-level library.*

1.  **Conceptual Setup:** Imagine you have a small dataset of customer queries labeled with their intent (e.g., "billing_inquiry", "technical_support").
2.  **Choose a Library:** Research libraries like `huggingface/peft` or `OpenPrompt` that provide implementations of prompt tuning.
3.  **Define a Soft Prompt:** Using the library's API, define a soft prompt of a certain length (e.g., 20-50 virtual tokens).
4.  **Conceptual Training:** Configure the training loop to only update the parameters of the soft prompt, keeping the base LLM frozen. Train on your intent classification dataset.
5.  **Conceptual Inference:** After training, use the learned soft prompt by prepending its embeddings to new customer queries. Observe if the model correctly classifies the intent.
6.  **Compare:** Reflect on how this approach differs from:
    *   Manually crafting a discrete prompt for intent classification.
    *   Fully fine-tuning the entire LLM for intent classification.

## Reflection

*   How do PEFT methods like prefix tuning and adapters offer a middle ground between zero-shot prompting and full fine-tuning?
*   What are the main advantages of using a soft prompt over a manually crafted discrete prompt for a specific task?
*   In what scenarios would the interpretability (or lack thereof) of soft prompts be a significant concern?
*   How might the ability to train small, task-specific modules (like adapters) change the way LLM applications are developed and deployed in the future?

## Limitations and Considerations

*   **Interpretability:** The learned "prompts" (vectors) are not human-readable, making it harder to understand *why* the model behaves a certain way.
*   **Requires Labeled Data:** Unlike zero-shot or few-shot prompting, PEFT methods still require a labeled dataset for training, albeit a smaller one than full fine-tuning.
*   **Training Infrastructure:** While less demanding than full fine-tuning, PEFT still requires a machine learning training setup (GPUs, frameworks).
*   **Performance Gap:** While often close, PEFT methods might not always match the peak performance of full fine-tuning for extremely complex or novel tasks.
*   **Model Compatibility:** Not all LLMs are equally amenable to all PEFT methods.

## Best Practices for Using PEFT

*   **Start with Discrete Prompts:** For initial exploration and prototyping, stick to manual prompt engineering.
*   **Evaluate Need:** Only consider PEFT when discrete prompting isn't sufficient, and full fine-tuning is too costly or complex.
*   **Choose Wisely:** Select the PEFT method that best fits your specific needs regarding parameter efficiency, modularity, and latency.
*   **Monitor Performance:** Track the performance of your PEFT-tuned models rigorously, just as you would with fully fine-tuned models.
*   **Leverage Libraries:** Use established libraries like Hugging Face's PEFT library to simplify implementation.
