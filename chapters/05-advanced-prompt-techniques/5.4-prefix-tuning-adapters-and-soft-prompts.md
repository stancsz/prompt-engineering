# 5.4 Prefix Tuning, Adapters, and Soft Prompts

Lightweight methods to steer large models without full fine-tuning.

## Prefix Tuning

- **Definition:** Learn a small continuous “prefix” of key/value pairs prepended to each layer’s input.  
- **Benefit:** Keeps the main model frozen; only ~0.1% of parameters are trained.  

### Example Workflow

1. Initialize a trainable prefix (length = 20 tokens).  
2. Optimize prefix on task data (e.g., classification).  
3. At inference, prepend learned prefix to every prompt.

## Adapters

- **Definition:** Insert trainable bottleneck modules into each transformer layer.  
- **Benefit:** Task-specific adapters can be stacked or shared across tasks.  

### Example Hands-On

```python
from transformers import AutoModel, AdapterConfig

model = AutoModel.from_pretrained("bert-base-uncased")
config = AdapterConfig(mh_adapter=True, output_adapter=True, reduction_factor=16)
model.add_adapter("sentiment", config)
model.train_adapter("sentiment")
```

## Soft Prompts

- **Definition:** Learn embeddings for prompt tokens directly, tuning them end-to-end.  
- **Benefit:** Flexible continuous prompts; do not map to human-readable text.

### Hands-On Exercise

1. Use OpenPrompt or Prompt Tuning library to define a soft prompt of length = 50.  
2. Fine-tune on small intent-classification dataset.  
3. Compare performance vs. manual discrete prompts.

## Reflection

- How much did parameter budget shrink vs. full fine-tuning?  
- Which method gave the best trade-off between speed and accuracy?
