# 5.4 Parameter-Efficient Fine-Tuning (PEFT): Prefix Tuning, Adapters, and Soft Prompts

Large Language Models (LLMs) are incredibly powerful, but fully fine-tuning them for every new task is computationally expensive, requires large datasets, and creates a new, large model checkpoint for each task. **Parameter-Efficient Fine-Tuning (PEFT)** methods offer a lightweight alternative, allowing you to adapt LLMs to specific tasks or domains by training only a small fraction of the model's parameters. This chapter explores key PEFT techniques like Prefix Tuning, Adapters, and Soft Prompts, and their relevance to prompt engineering.

## Why PEFT Methods Matter

*   **Reduced Computational Cost:** Train only a small number of parameters, significantly lowering GPU memory and compute requirements.
*   **Faster Training:** Training converges much quicker than full fine-tuning.
*   **Storage Efficiency:** Store only small, task-specific "adapter" modules or "soft prompts" instead of entire model copies.
*   **Modularity:** Easily swap out task-specific modules without reloading the entire base model.
*   **Avoid Catastrophic Forgetting:** The frozen base model retains its general knowledge, preventing degradation on other tasks.

## Key PEFT Techniques

### 1. Prefix Tuning

**Definition:** Prefix tuning involves learning a small, continuous sequence of task-specific vectors (the "prefix") that are prepended to the input embeddings at each layer of the Transformer model. The base LLM's parameters remain frozen; only these prefix vectors are trained.

*   **Mechanism:** Instead of adding discrete, human-readable tokens to the prompt, prefix tuning adds a learned, continuous "virtual prompt" in the model's embedding space. This prefix guides the model's internal representations to steer its behavior towards the desired task.
*   **Analogy:** Imagine you're giving the model a "mental nudge" or a "pre-computation" at every layer, rather than just at the input.
*   **Benefits:** Highly parameter-efficient (often training <0.1% of total parameters), effective for various NLP tasks.
*   **Limitations:** The learned prefix is not human-interpretable. Requires a training dataset for each task.

**Example Workflow (Conceptual Training & Inference):**

1.  **Initialization:** Initialize a small, trainable matrix of vectors (the "prefix") for each Transformer layer.
2.  **Training:** For a specific task (e.g., sentiment classification), feed labeled data through the LLM. During backpropagation, only the parameters of the prefix vectors are updated, while the main LLM weights are frozen.
3.  **Inference:** When making a prediction, the learned prefix vectors are prepended to the input embeddings of the prompt at each layer, guiding the LLM to produce the desired output for that task.

### 2. Adapters (Adapter-based Fine-Tuning)

**Definition:** Adapters are small, lightweight neural network modules inserted *within* each layer of a pre-trained Transformer model. During fine-tuning, only the parameters of these adapter modules are trained, while the original LLM weights remain frozen.

*   **Mechanism:** An adapter typically consists of a down-projection, a non-linear activation, and an up-projection, creating a bottleneck. This bottleneck allows the adapter to learn task-specific transformations with very few parameters.
*   **Benefits:**
    *   **Modularity:** Different tasks can have their own adapter modules, which can be easily swapped in and out.
    *   **Composability:** Multiple adapters can be stacked or combined for multi-task learning or sequential task execution.
    *   **Parameter Efficiency:** Similar to prefix tuning, only a tiny fraction of parameters are trained.
*   **Limitations:** Can add a slight increase in inference latency due to the additional layers.

**Example Hands-On (using `huggingface/adapters` library):**

```python
# This is a conceptual example. Full setup requires dataset loading and training loop.
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdapterConfig
from datasets import load_dataset

# 1. Load a pre-trained model and tokenizer
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# 2. Define an Adapter configuration
# This creates a bottleneck adapter with a reduction factor of 16
adapter_config = AdapterConfig.load("pfeiffer", reduction_factor=16)

# 3. Add a new adapter for a specific task (e.g., sentiment analysis)
model.add_adapter("sentiment_analysis", config=adapter_config)

# 4. Set the adapter to be active for training
model.train_adapter("sentiment_analysis")

# 5. (Conceptual) Train the model on a sentiment dataset
# This would involve preparing a dataset, defining a training loop,
# and calling model.train() or using a Trainer from Hugging Face.
# During training, only the adapter parameters are updated.

# 6. (Conceptual) Save and Load the adapter
# model.save_adapter("./my_sentiment_adapter", "sentiment_analysis")
# model.load_adapter("./my_sentiment_adapter", "sentiment_analysis")

# 7. (Conceptual) Activate the adapter for inference
# model.set_active_adapter("sentiment_analysis")
# Then, use the model for sentiment prediction.
```

### 3. Soft Prompts (Prompt Tuning / Prompt Learning)

**Definition:** Soft prompts (or prompt tuning/prompt learning) is a broader category of PEFT methods where continuous, trainable vectors are learned and prepended to the input sequence, similar to prefix tuning. The key distinction is that these "prompts" are not human-readable tokens but rather optimized numerical embeddings.

*   **Mechanism:** Instead of manually crafting discrete text prompts, you define a fixed-length sequence of "virtual tokens" whose embeddings are randomly initialized and then optimized through backpropagation on a downstream task. These learned embeddings are then prepended to the input embeddings of the actual text.
*   **Benefits:** Extremely parameter-efficient, can achieve performance comparable to full fine-tuning on many tasks, and are highly flexible.
*   **Limitations:** The learned soft prompt is not human-interpretable. Requires a training dataset.

**Conceptual Visualization of Soft Prompts:**

```mermaid
graph TD
    A[Input Text] --> B(Tokenization);
    B --> C[Input Embeddings];
    D[Learned Soft Prompt Embeddings] --> E(Concatenation);
    C --> E;
    E --> F[LLM Transformer Layers (Frozen)];
    F --> G[Output];

    style D fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

**Example Workflow (Conceptual Training & Inference):**

1.  **Define Soft Prompt Length:** Choose a length for your soft prompt (e.g., 10-100 virtual tokens). This length is a hyperparameter.
2.  **Initialize:** Create a trainable tensor of embeddings for these virtual tokens. These are typically randomly initialized.
3.  **Training:** During training, these soft prompt embeddings are concatenated with the embeddings of your input text. The combined sequence is then fed through the LLM. Crucially, only the soft prompt embeddings are updated via backpropagation, while the main LLM weights are frozen.
4.  **Inference:** The learned soft prompt embeddings are prepended to the input embeddings of new queries. This "virtual prompt" guides the LLM's response towards the specific task it was tuned for, without altering the core model.

## Comparison and When to Use Which

PEFT methods offer a spectrum of trade-offs between performance, efficiency, and complexity. Understanding these differences helps in choosing the right approach for your application.

| Feature             | Full Fine-Tuning                               | Prefix Tuning / Soft Prompts                     | Adapters                                         |
| :------------------ | :--------------------------------------------- | :----------------------------------------------- | :----------------------------------------------- |
| **Parameters Trained** | All LLM parameters                             | ~0.1% of LLM parameters (prefix/soft prompt)     | ~0.1-5% of LLM parameters (adapter modules)      |
| **Storage Cost**    | High (new full model checkpoint per task)      | Low (small prefix/adapter weights per task)      | Low (small adapter weights per task)             |
| **Training Speed**  | Slow                                           | Fast                                             | Fast                                             |
| **Inference Latency** | Base model latency                             | Base model latency (minimal overhead)            | Slightly increased (due to adapter layers)       |
| **Interpretability**| N/A                                            | Low (learned vectors)                            | Low (learned weights in bottleneck layers)       |
| **Modularity**      | Low (new model per task)                       | High (small prefixes can be swapped)             | Very High (adapters can be stacked/swapped)      |
| **Best For**        | Significant domain adaptation, high performance on specific, complex tasks | Many diverse tasks, resource-constrained environments | Multi-task learning, composable AI systems       |

**When to Choose:**

*   **Full Fine-Tuning:** When you need the absolute highest performance for a critical task, have a large, high-quality labeled dataset, and can afford the significant computational resources and storage. *Example Use Case: Achieving state-of-the-art accuracy on a highly specialized medical text classification task with a large proprietary dataset.*
*   **Prefix Tuning / Soft Prompts:** When you have many diverse tasks and want to adapt a single base LLM to all of them efficiently, or when you are resource-constrained. They are excellent for classification, summarization, and question-answering tasks where you want to "steer" the model's behavior without altering its core knowledge. *Example Use Case: Adapting a general-purpose LLM to classify customer support tickets into dozens of categories, or generating concise summaries of legal documents.*
*   **Adapters:** Similar to prefix tuning in efficiency, but offers more modularity and composability. This makes them ideal for scenarios where you might want to combine multiple task-specific behaviors or build systems where different components can be independently updated. *Example Use Case: Building a multi-task chatbot where different adapters handle intent recognition, entity extraction, and response generation, or creating a system where new tasks can be added by simply plugging in a new adapter.*
*   **Prompt Engineering (Discrete Prompts):** For rapid prototyping, tasks that don't require extreme specialization, or when you don't have labeled data for training. This is the most flexible and immediate approach for many common LLM use cases. *Example Use Case: Quickly building a content generation tool for marketing copy, or a simple Q&A system for publicly available information.*

| Feature             | Full Fine-Tuning                               | Prefix Tuning / Soft Prompts                     | Adapters                                         |
| :------------------ | :--------------------------------------------- | :----------------------------------------------- | :----------------------------------------------- |
| **Parameters Trained** | All LLM parameters                             | ~0.1% of LLM parameters (prefix/soft prompt)     | ~0.1-5% of LLM parameters (adapter modules)      |
| **Storage Cost**    | High (new full model checkpoint per task)      | Low (small prefix/adapter weights per task)      | Low (small adapter weights per task)             |
| **Training Speed**  | Slow                                           | Fast                                             | Fast                                             |
| **Inference Latency** | Base model latency                             | Base model latency (minimal overhead)            | Slightly increased (due to adapter layers)       |
| **Interpretability**| N/A                                            | Low (learned vectors)                            | Low (learned weights in bottleneck layers)       |
| **Modularity**      | Low (new model per task)                       | High (small prefixes can be swapped)             | Very High (adapters can be stacked/swapped)      |
| **Best For**        | Significant domain adaptation, high performance on specific, complex tasks | Many diverse tasks, resource-constrained environments | Multi-task learning, composable AI systems       |

**When to Choose:**

*   **Full Fine-Tuning:** When you need the absolute highest performance for a critical task, have a large, high-quality labeled dataset, and can afford the computational resources and storage.
*   **Prefix Tuning / Soft Prompts:** When you have many tasks and want to adapt a single base LLM to all of them efficiently, or when you are resource-constrained.
*   **Adapters:** Similar to prefix tuning, but offers more modularity and composability, making it ideal for scenarios where you might want to combine multiple task-specific behaviors.
*   **Prompt Engineering (Discrete Prompts):** For rapid prototyping, tasks that don't require extreme specialization, or when you don't have labeled data for training. This is the most flexible and immediate approach.

## Hands-On Exercise: Conceptual Prompt Tuning with Hugging Face PEFT

*Note: Fully implementing and training a PEFT model requires a proper machine learning setup (e.g., PyTorch/TensorFlow, a dataset, and a training loop). This conceptual exercise focuses on demonstrating the setup steps using the `huggingface/peft` library.*

**Setup:**
1.  **Python Environment:** Ensure you have Python 3.8+ installed.
2.  **Install Libraries:**
    ```bash
    pip install transformers peft datasets accelerate
    ```
    *   `transformers`: For accessing pre-trained LLMs.
    *   `peft`: Hugging Face's Parameter-Efficient Fine-Tuning library.
    *   `datasets`: For easy dataset loading (conceptual here).
    *   `accelerate`: For simplified distributed training (often used with PEFT).

**Conceptual `peft_prompt_tuning_example.py`:**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup
from peft import get_peft_config, PeftModel, PeftConfig, LoraConfig, TaskType, PromptTuningConfig, PromptTuningInit
from torch.utils.data import DataLoader
from datasets import load_dataset
from tqdm import tqdm
import torch

# 1. Load a pre-trained model and tokenizer (e.g., a small GPT-2 for demonstration)
model_name_or_path = "gpt2"
tokenizer_name_or_path = "gpt2"

tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
# GPT-2 doesn't have a default pad token, which is needed for batching
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

model = AutoModelForCausalLM.from_pretrained(model_name_or_path)

# 2. Define a Prompt Tuning configuration
# We'll use PromptTuning for this example
peft_config = PromptTuningConfig(
    task_type=TaskType.CAUSAL_LM, # Or TaskType.SEQ_CLS for classification
    prompt_tuning_init=PromptTuningInit.TEXT, # Initialize with text or random
    num_virtual_tokens=20, # Length of the soft prompt
    prompt_tuning_init_text="Classify the sentiment of this review: ", # Initial text for virtual tokens
    tokenizer_name_or_path=tokenizer_name_or_path,
)

# 3. Get the PEFT model
# This wraps the base model and adds the trainable soft prompt
peft_model = PeftModel(model, peft_config)
peft_model.print_trainable_parameters() # Shows only the soft prompt parameters are trainable

# 4. (Conceptual) Prepare a Dataset for a specific task (e.g., sentiment classification)
# In a real scenario, you would load and preprocess your actual dataset.
# Here, we simulate a very small dataset.
print("\n--- Conceptual Dataset Preparation ---")
raw_datasets = load_dataset("imdb") # Using a public dataset for conceptual demo
# Take a very small subset for quick conceptual run
train_dataset = raw_datasets["train"].select(range(100))
eval_dataset = raw_datasets["test"].select(range(20))

def tokenize_function(examples):
    # For causal LM, we concatenate input and label for training
    # For classification, you'd tokenize input and have a separate label
    return tokenizer(examples["text"], truncation=True, max_length=128)

tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)

# Remove original text column as it's no longer needed after tokenization
tokenized_train_dataset = tokenized_train_dataset.remove_columns(["text"])
tokenized_eval_dataset = tokenized_eval_dataset.remove_columns(["text"])

# For causal LM, labels are usually the same as input_ids
tokenized_train_dataset = tokenized_train_dataset.rename_column("label", "labels")
tokenized_eval_dataset = tokenized_eval_dataset.rename_column("label", "labels")


# 5. (Conceptual) Define a Training Loop
# This is a simplified training loop. In practice, you'd use Hugging Face Trainer or Accelerate.
print("\n--- Conceptual Training Loop (Simplified) ---")
optimizer = torch.optim.AdamW(peft_model.parameters(), lr=1e-3)
train_dataloader = DataLoader(tokenized_train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=4)

peft_model.train()
for epoch in range(1): # Train for 1 epoch for conceptual demo
    for batch in tqdm(train_dataloader, desc="Training"):
        # Move batch to device (e.g., GPU if available)
        batch = {k: v.to(peft_model.device) for k, v in batch.items()}
        outputs = peft_model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    print(f"Epoch 1 Loss: {loss.item():.4f}")

# 6. (Conceptual) Inference with the learned soft prompt
print("\n--- Conceptual Inference ---")
peft_model.eval()
input_text = "This movie was absolutely fantastic and I loved every minute of it."
# The soft prompt is automatically prepended by the PeftModel during inference
inputs = tokenizer(input_text, return_tensors="pt").to(peft_model.device)

with torch.no_grad():
    outputs = peft_model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=50, # Generate a short continuation
        do_sample=True,
        temperature=0.7
    )
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"Input: {input_text}")
print(f"Generated continuation (with soft prompt influence): {generated_text}")

# Compare to baseline (without PEFT model)
print("\n--- Baseline Inference (without PEFT) ---")
model.eval()
with torch.no_grad():
    baseline_outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=50,
        do_sample=True,
        temperature=0.7
    )
baseline_generated_text = tokenizer.decode(baseline_outputs[0], skip_special_tokens=True)
print(f"Input: {input_text}")
print(f"Generated continuation (baseline): {baseline_generated_text}")

```

**To Run This Exercise:**
1.  Save the code above as `peft_prompt_tuning_example.py` in the chapter directory.
2.  Open your terminal in the `chapters/05-advanced-prompt-techniques/` directory.
3.  Run: `python peft_prompt_tuning_example.py`

**Reflection:**
*   How do PEFT methods like prefix tuning and adapters offer a middle ground between zero-shot prompting and full fine-tuning?
*   What are the main advantages of using a soft prompt over a manually crafted discrete prompt for a specific task?
*   In what scenarios would the interpretability (or lack thereof) of soft prompts be a significant concern?
*   How might the ability to train small, task-specific modules (like adapters) change the way LLM applications are developed and deployed in the future?

## Reflection

*   How do PEFT methods like prefix tuning and adapters offer a middle ground between zero-shot prompting and full fine-tuning?
*   What are the main advantages of using a soft prompt over a manually crafted discrete prompt for a specific task?
*   In what scenarios would the interpretability (or lack thereof) of soft prompts be a significant concern?
*   How might the ability to train small, task-specific modules (like adapters) change the way LLM applications are developed and deployed in the future?

## Limitations and Considerations

*   **Interpretability:** The learned "prompts" (vectors) are not human-readable, making it harder to understand *why* the model behaves a certain way.
*   **Requires Labeled Data:** Unlike zero-shot or few-shot prompting, PEFT methods still require a labeled dataset for training, albeit a smaller one than full fine-tuning.
*   **Training Infrastructure:** While less demanding than full fine-tuning, PEFT still requires a machine learning training setup (GPUs, frameworks).
*   **Performance Gap:** While often close, PEFT methods might not always match the peak performance of full fine-tuning for extremely complex or novel tasks.
*   **Model Compatibility:** Not all LLMs are equally amenable to all PEFT methods.

## Best Practices for Using PEFT

*   **Start with Discrete Prompts:** For initial exploration and prototyping, stick to manual prompt engineering.
*   **Evaluate Need:** Only consider PEFT when discrete prompting isn't sufficient, and full fine-tuning is too costly or complex.
*   **Choose Wisely:** Select the PEFT method that best fits your specific needs regarding parameter efficiency, modularity, and latency.
*   **Monitor Performance:** Track the performance of your PEFT-tuned models rigorously, just as you would with fully fine-tuned models.
*   **Leverage Libraries:** Use established libraries like Hugging Face's PEFT library to simplify implementation.
