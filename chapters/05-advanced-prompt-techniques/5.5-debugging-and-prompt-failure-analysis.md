# 5.5 Debugging and Prompt Failure Analysis

Prompt engineering is an iterative process. Just like software development, prompts rarely work perfectly on the first try. Understanding **why prompts fail** and having a systematic approach to **debugging** them is a critical skill for any prompt engineer. This chapter outlines common failure modes and provides a structured methodology for analyzing and resolving prompt-related issues.

## Common Prompt Failure Modes

Identifying the type of failure is the first step in debugging.

1.  **Hallucinations:**
    *   **Description:** The model generates factually incorrect, nonsensical, or fabricated information with high confidence.
    *   **Causes:** Lack of sufficient grounding context, over-reliance on internal knowledge for niche topics, high temperature settings, or ambiguous prompts.
    *   **Example:** Asking about a non-existent product feature, or providing incorrect historical dates.

2.  **Truncation:**
    *   **Description:** The model's output is cut off prematurely, or it fails to consider all provided input context.
    *   **Causes:** Exceeding the LLM's context window limit (input + output tokens).
    *   **Example:** A summary that ends abruptly, or a response that ignores the latter half of a long document.

3.  **Ambiguity/Misinterpretation:**
    *   **Description:** The model misunderstands the intent of the prompt, leading to irrelevant, off-topic, or inconsistent responses.
    *   **Causes:** Vague instructions, unclear phrasing, conflicting constraints, or implicit assumptions in the prompt.
    *   **Example:** Asking for "key points" and getting a full summary instead of bullet points, or a chatbot responding generically when a specific action was implied.

4.  **Overfitting to Examples (in Few-Shot):**
    *   **Description:** The model mimics the provided few-shot examples too literally, failing to generalize to new inputs or adopting an undesired style from the examples.
    *   **Causes:** Examples are not diverse enough, too few examples, or examples contain subtle biases/patterns that are unintentionally learned.
    *   **Example:** A summarization prompt with examples always produces summaries of the same length, even when instructed otherwise for a new input.

5.  **Irrelevant Output:**
    *   **Description:** The model generates text that is coherent but does not directly address the user's query or the prompt's objective.
    *   **Causes:** Insufficient constraints, lack of a clear call to action, or the model "drifting" off-topic.

6.  **Repetition:**
    *   **Description:** The model generates repetitive phrases, sentences, or patterns.
    *   **Causes:** Low temperature settings, lack of diversity in decoding (e.g., too low top-k/top-p), or inherent model biases.

7.  **Format Mismatch:**
    *   **Description:** The model fails to adhere to specified output formats (e.g., JSON, bullet points, tables).
    *   **Causes:** Unclear format instructions, complex formats, or the model prioritizing content over structure.

8.  **Safety/Bias Issues:**
    *   **Description:** The model generates harmful, biased, or inappropriate content.
    *   **Causes:** Biases in training data, prompt injection (Chapter 9.1), or insufficient safety guardrails.

## Systematic Debugging Steps

Debugging prompts is an iterative process of hypothesis, experimentation, and observation.

1.  **Reproduce the Failure Consistently:**
    *   Run the problematic prompt multiple times (e.g., 5-10 times) with the *same* decoding parameters (especially `temperature=0.0` or a fixed `seed` if available) to confirm the failure is consistent and not a random fluctuation.
    *   If it's inconsistent, the issue might be related to randomness (temperature) or the model's inherent variability.

2.  **Isolate Variables (One Change at a Time):**
    *   Change only one element of the prompt or decoding parameter per test. This helps pinpoint the root cause.
    *   **Prompt Elements to Vary:**
        *   **Role:** Remove or change the assigned persona.
        *   **Instructions:** Simplify, make more explicit, add/remove constraints.
        *   **Context:** Add more, remove some, or rephrase.
        *   **Examples (Few-Shot):** Add more, remove some, change their order, or ensure diversity.
        *   **Delimiters:** Experiment with different ways to separate instructions from input.
        *   **Negative Constraints:** Add "do not" instructions.
    *   **Decoding Parameters to Vary:**
        *   **Temperature:** Increase for creativity, decrease for determinism.
        *   **Top-k / Top-p:** Adjust to control diversity.
        *   **Max Tokens:** Ensure it's sufficient for the desired output.

3.  **Inspect Intermediate Outputs (for Chained Prompts/CoT):**
    *   If you're using prompt chaining (Chapter 5.1) or Chain-of-Thought (Chapter 4.2), examine the output of each individual step.
    *   An error in an early stage will propagate. Pinpoint where the reasoning or data transformation first goes wrong.

4.  **Simplify the Prompt:**
    *   If the prompt is complex, try removing non-essential parts (e.g., extra context, secondary instructions, specific formatting requests).
    *   Start with the bare minimum instruction and gradually add elements back until the failure reappears. This helps identify the problematic component.

5.  **Check Token Count:**
    *   Use a tokenizer tool (Chapter 2.3) to verify that your prompt (including all context and examples) does not exceed the model's context window.
    *   If it does, implement context management strategies (Chapter 4.4) like summarization or chunking.

6.  **Review Model Documentation:**
    *   Different LLMs have different strengths, weaknesses, and optimal prompting styles. Consult the model's official documentation for best practices and known limitations.

7.  **Use Evaluation Metrics:**
    *   For tasks with objective answers (e.g., classification, factual Q&A), use automated metrics (Chapter 6.1) to quantify performance changes after each prompt iteration.
    *   For subjective tasks, conduct human evaluations (Chapter 6.2).

## Example Debugging Session: Addressing Inconsistent Factual Recall

**Problem:** A prompt designed to extract a specific historical fact sometimes gives the correct answer, sometimes hallucinates, and sometimes says it doesn't know.

**Initial Prompt:**
```
You are a history expert. What year was the Battle of Hastings fought?
```

**Debugging Steps:**

1.  **Reproduce:** Run 10 times with `temperature=0.7`. Observe 6 correct answers (1066), 2 incorrect (e.g., 1070), and 2 "I don't know." *Conclusion: Inconsistent, needs improvement.*

2.  **Isolate - Add Specificity:**
    *   Hypothesis: The model might be too general.
    *   Change: Add a constraint for direct answer.
    *   New Prompt: `You are a history expert. What year was the Battle of Hastings fought? Answer only with the year.`
    *   Result: Still inconsistent, but fewer "I don't know." *Conclusion: Better, but not solved.*

3.  **Isolate - Add CoT:**
    *   Hypothesis: Model needs to "think" more.
    *   Change: Add "Let's think step by step."
    *   New Prompt: `You are a history expert. What year was the Battle of Hastings fought? Let's think step by step and then state the year.`
    *   Result: Model now provides reasoning (e.g., "William the Conqueror invaded England..."), but still occasionally gets the year wrong in the final step. *Conclusion: Reasoning is better, but still prone to error.*

4.  **Isolate - Add Context (RAG-like):**
    *   Hypothesis: Model lacks specific grounding.
    *   Change: Provide a short, factual snippet about the battle.
    *   New Prompt:
        ```
        You are a history expert. Based on the following text, what year was the Battle of Hastings fought? Answer only with the year.

        Text: "The Battle of Hastings was a pivotal event in English history, fought on 14 October 1066 between the Norman-French army of William, Duke of Normandy, and an English army under the Anglo-Saxon King Harold Godwinson."
        ```
    *   Result: Consistently returns `1066`. *Conclusion: Providing explicit, grounded context resolves the hallucination and inconsistency.*

## Hands-On Exercise: Debugging a Formatting Issue

1.  **Problematic Prompt:** You want to extract names and emails from a list of contacts and output them as a JSON array of objects.
    ```
    Extract names and emails from the following contacts and return as a JSON array of objects.

    Contacts:
    John Doe, john.doe@example.com
    Jane Smith (jane.smith@company.org)
    Alice Brown <alice.brown@mail.net>
    ```
    *   Run this prompt several times. You might observe inconsistent JSON formatting (e.g., missing commas, incorrect keys, not a valid array).

2.  **Apply Debugging Steps:**
    *   **Reproduce:** Confirm the inconsistency.
    *   **Isolate - Add Examples (Few-Shot):** Provide one or two perfect examples of the desired JSON output.
    *   **Isolate - Add Delimiters:** Wrap the `Contacts` section in triple backticks or XML tags.
    *   **Isolate - Be More Explicit about JSON:** Add instructions like "Ensure the output is valid JSON."
    *   **Isolate - Role Assignment:** Add "You are a JSON formatter and data extractor."
    *   **Simplify:** Try with just one contact first.

3.  **Document and Reflect:** For each change, note how the output's JSON validity and consistency improve (or worsen). Which combination of changes yielded the most robust JSON output?

## Tools for Debugging

*   **LLM Playgrounds:** Provide immediate feedback and allow quick iteration.
*   **Prompt Management Platforms:** Tools like Weights & Biases Prompts, LangChain Hub, or custom internal systems can help version control prompts, track experiments, and compare outputs.
*   **Logging and Monitoring:** Essential in production to capture LLM inputs, outputs, latency, and errors for post-mortem analysis.
*   **Automated Evaluation Frameworks:** (Covered in Chapter 6) Automate the process of checking output quality against predefined criteria.

## Best Practices for Robust Prompting

*   **Be Explicit, Not Implicit:** Always assume the model needs clear instructions.
*   **Use Delimiters:** Clearly separate instructions from dynamic content.
*   **Provide Examples:** For complex formats or nuanced tasks, few-shot examples are invaluable.
*   **Ground with Context (RAG):** For factual tasks, retrieve and inject relevant information.
*   **Iterate Systematically:** Follow a structured debugging process.
*   **Validate Outputs:** Implement programmatic checks on LLM outputs (e.g., JSON schema validation).
*   **Monitor in Production:** Track performance metrics and user feedback to identify new failure modes.
*   **Consider Model Limitations:** Understand that no prompt can make a model perform beyond its inherent capabilities.
