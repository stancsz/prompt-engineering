# 5.2 Retrieval-Augmented Generation & Memory

Augmenting prompts with external context—either by retrieving documents or by keeping “memory”—lets you ground the model in up-to-date information.

## Concept

- **Retrieval-Augmented Generation (RAG):**  
  1. Embed user query.  
  2. Search a document store (vector database).  
  3. Prepend retrieved passages to the prompt.  
- **Memory:**  
  - Short-term: carry context across turns in a chat session.  
  - Long-term: store embeddings or summaries of past interactions to inform future queries.

## Example Workflow

1. **Indexing:**  
   - Embed 1,000 FAQs with OpenAI embeddings.  
   - Store vectors in Pinecone or Elasticsearch.  
2. **Query:**  
   ```
   User: “How do I reset my password?”  
   ```  
3. **Retrieval:**  
   - Find top 3 similar FAQs.  
4. **Prompt:**  
   ```
   You are a support agent. Based on these documents:
   [FAQ 1 excerpt]
   [FAQ 2 excerpt]
   [FAQ 3 excerpt]

   Answer: “How do I reset my password?”
   ```
5. **Generation:** model provides grounded answer.

## Hands-On Exercise

1. Choose 10 articles or support documents.  
2. Generate embeddings (example in Python):  
   ```python
   from openai import OpenAI
   client = OpenAI()
   vectors = [client.embeddings.create(input=doc) for doc in docs]
   ```  
3. Load vectors into a simple in-memory search (e.g., scikit-learn NearestNeighbors).  
4. Given a query prompt, retrieve top 3 passages and format a RAG prompt.  
5. Send to an LLM and compare against no-retrieval baseline.

## Reflection

- How did retrieval improve relevance and accuracy?  
- What trade-offs arose from longer prompts?  
- How might you compress or summarize retrieved context to fit token limits?
