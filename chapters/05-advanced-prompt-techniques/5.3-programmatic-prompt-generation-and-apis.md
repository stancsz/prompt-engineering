# 5.3 Programmatic Prompt Generation & APIs

Generating prompts programmatically enables dynamic, data-driven workflows and integration into production systems.

## Concept

- **Template Engines:** Use string templates (e.g., Jinja, Mustache) to fill variables.  
- **Prompt SDKs:** Libraries (LangChain, OpenAI SDK) for chaining, caching, and batching prompts.  
- **API Integration:** Call model endpoints from code, handle retries, rate limits, and parsing.

## Example (Python + Jinja)

```python
from jinja2 import Template
from openai import OpenAI

template = Template("You are a {{ role }}. Summarize: '{{ text }}' in {{ bullets }} bullets.")
prompt = template.render(role="research assistant", text=abstract, bullets=3)

client = OpenAI()
response = client.chat.completions.create(model="gpt-4", messages=[{"role": "user", "content": prompt}])
print(response.choices[0].message.content)
```

## Hands-On Exercise

1. Install Jinja2 and OpenAI SDK (`pip install jinja2 openai`).  
2. Write a script that:
   - Reads a CSV of product descriptions.
   - Uses a prompt template to generate marketing copy for each row.
   - Saves outputs to a new CSV.  
3. Test error handling by simulating an API failure (invalid key) and retrying once.

## Reflection

- How did templating improve consistency and speed?  
- Where did you need custom logic (e.g., sanitizing inputs)?  
- How would you extend this pattern for batch processing or different models?
