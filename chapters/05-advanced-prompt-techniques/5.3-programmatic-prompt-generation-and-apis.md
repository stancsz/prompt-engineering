# 5.3 Programmatic Prompt Generation & API Integration

While manual prompt engineering in playgrounds is excellent for experimentation, building real-world Large Language Model (LLM) applications requires **programmatic prompt generation** and robust **API integration**. This involves writing code to dynamically construct prompts, send them to LLM APIs, and process their responses. This approach enables automation, scalability, data-driven workflows, and seamless integration into existing software systems.

## Concept: Automating LLM Interactions

### 1. Prompt Templating Engines

**Role:** These tools allow you to define prompt structures with placeholders (variables) that are filled with dynamic content at runtime. This is a programmatic implementation of the "Static Templates and Dynamic Variables" pattern (Chapter 4.1).

*   **Mechanism:** You define a template string (e.g., `You are a {role}. Summarize this: {text}`). Your code then populates these placeholders with actual data before sending the complete prompt to the LLM.
*   **Benefits:** Ensures consistency, reusability, and maintainability of prompts across an application.
*   **Popular Libraries:**
    *   **Jinja2 (Python):** Widely used for templating in Python web frameworks and general text generation.
    *   **Mustache/Handlebars (JavaScript/various):** Cross-language templating systems.
    *   **F-strings (Python):** Simple string formatting for basic templating.

### 2. LLM SDKs and Libraries

**Role:** Software Development Kits (SDKs) provided by LLM providers (e.g., OpenAI Python SDK, Google Cloud Client Libraries) or higher-level frameworks (e.g., LangChain, Semantic Kernel) abstract away the complexities of direct API calls.

*   **Mechanism:** These SDKs provide convenient functions and classes to:
    *   Interact with LLM endpoints (e.g., `client.chat.completions.create()`).
    *   Manage prompt formatting (e.g., handling chat message roles like "system," "user," "assistant").
    *   Implement common patterns like prompt chaining, caching, and batching.
    *   Integrate with other components (e.g., vector databases for RAG).
*   **Benefits:** Simplifies development, reduces boilerplate code, and often includes built-in best practices for API interaction.

### 3. Direct API Integration

**Role:** Directly making HTTP requests to LLM endpoints. While SDKs are preferred, understanding direct API integration is useful for debugging, custom implementations, or when an SDK is not available.

*   **Mechanism:** Sending JSON payloads via HTTP POST requests to a specified API endpoint, including authentication headers (e.g., API keys).
*   **Benefits:** Maximum control over requests and responses.
*   **Considerations:** Requires manual handling of authentication, request formatting, response parsing, error handling, and retry logic.

## Benefits of Programmatic Prompting

*   **Automation:** Automate repetitive tasks (e.g., generating thousands of product descriptions).
*   **Scalability:** Handle high volumes of requests by integrating LLMs into scalable backend services.
*   **Dynamic Workflows:** Create prompts that adapt based on real-time data, user input, or application state.
*   **Integration:** Embed LLM capabilities directly into existing software applications, databases, and workflows.
*   **Version Control:** Manage prompts as code, allowing for versioning, collaboration, and CI/CD practices.
*   **Error Handling & Robustness:** Implement sophisticated error handling, retry mechanisms, and input/output validation.

## Key Considerations for Production Systems

*   **Authentication and Security:** Securely manage API keys (e.g., environment variables, secret management services).
*   **Rate Limits:** LLM APIs often have rate limits (requests per minute/second). Implement exponential backoff and retry logic to handle these.
*   **Cost Management:** Monitor token usage and costs. Optimize prompts for conciseness.
*   **Latency:** Minimize API call latency through efficient prompt design, batching, and asynchronous calls.
*   **Error Handling:** Anticipate and handle various API errors (e.g., invalid requests, server errors, rate limits).
*   **Input/Output Validation:** Validate inputs before sending to the LLM and parse/validate outputs to ensure they meet expected formats.
*   **Observability:** Implement logging, monitoring, and tracing to understand LLM performance and debug issues in production.

## Example: Dynamic Content Generation with Python & OpenAI API

This example demonstrates generating personalized email subject lines using a template and the OpenAI Python SDK.

```python
import os
from jinja2 import Template
from openai import OpenAI
import time
import json

# Ensure your OpenAI API key is set as an environment variable
# os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY" # Uncomment and replace for direct testing
client = OpenAI()

# 1. Define the Prompt Template
email_subject_template = Template("""
You are a marketing copywriter.
Generate an email subject line for a product launch.

Product Name: {{ product_name }}
Key Feature: {{ key_feature }}
Target Audience: {{ target_audience }}
Tone: {{ tone }}
Length: {{ length_constraint }}

Subject Line:
""")

# 2. Prepare Dynamic Data (e.g., from a database or user input)
product_data = [
    {
        "product_name": "QuantumFlow Smartwatch",
        "key_feature": "All-day battery life",
        "target_audience": "Fitness enthusiasts",
        "tone": "exciting",
        "length_constraint": "under 60 characters"
    },
    {
        "product_name": "EcoClean Laundry Pods",
        "key_feature": "Plant-based, zero waste",
        "target_audience": "Eco-conscious families",
        "tone": "informative and trustworthy",
        "length_constraint": "under 70 characters"
    }
]

# 3. Iterate and Generate Prompts & Call API
generated_subjects = []

for data in product_data:
    # Render the prompt with dynamic data
    prompt_content = email_subject_template.render(
        product_name=data["product_name"],
        key_feature=data["key_feature"],
        target_audience=data["target_audience"],
        tone=data["tone"],
        length_constraint=data["length_constraint"]
    )

    print(f"\n--- Generating for {data['product_name']} ---")
    print(f"Prompt:\n{prompt_content}")

    try:
        # Call the OpenAI API
        response = client.chat.completions.create(
            model="gpt-3.5-turbo", # Or "gpt-4" for higher quality
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt_content}
            ],
            temperature=0.7,
            max_tokens=50 # Limit response length to avoid excessive generation
        )
        subject_line = response.choices[0].message.content.strip()
        generated_subjects.append({
            "product": data["product_name"],
            "subject_line": subject_line
        })
        print(f"Generated Subject: {subject_line}")

    except Exception as e:
        print(f"Error generating subject for {data['product_name']}: {e}")
        # Implement retry logic here for production systems
    
    time.sleep(0.1) # Small delay to avoid hitting rate limits too quickly

print("\n--- All Generated Subject Lines ---")
print(json.dumps(generated_subjects, indent=2))
```

## Hands-On Exercise: Building a Programmatic Summarizer

1.  **Setup:**
    *   Ensure Python is installed.
    *   Install necessary libraries: `pip install openai jinja2 pandas`
    *   Set your `OPENAI_API_KEY` environment variable.
2.  **Create Input Data:** Create a CSV file named `articles.csv` with two columns: `id` and `text`. Populate it with a few short articles or paragraphs.
    ```csv
    id,text
    1,"The recent discovery of a new exoplanet, Kepler-186f, has excited astronomers. It is the first Earth-size planet found in the habitable zone of another star, suggesting it could potentially harbor liquid water and life. This finding opens new avenues for the search for extraterrestrial life."
    2,"Quantum computing is a rapidly emerging technology that harnesses the principles of quantum mechanics to solve problems too complex for classical computers. Unlike classical bits, which are either 0 or 1, quantum bits (qubits) can be both simultaneously, enabling exponential processing power."
    ```
3.  **Write the Python Script:** Create a Python file (e.g., `summarizer.py`) that:
    *   Reads `articles.csv` using pandas.
    *   Defines a Jinja2 template for summarization (e.g., "Summarize the following article in 3 bullet points: {article_text}").
    *   Iterates through each row of the DataFrame.
    *   Renders the prompt with the article text.
    *   Calls the OpenAI API to get a summary.
    *   Appends the generated summary to a list.
    *   Saves the original `id` and the `summary` to a new CSV file named `summaries.csv`.
    *   Include basic `try-except` blocks for API errors.
4.  **Run and Verify:** Execute your script (`python summarizer.py`). Check the `summaries.csv` file for the generated content.

## Reflection

*   How did programmatic generation improve the efficiency of processing multiple articles compared to manual prompting?
*   What challenges did you face in parsing the LLM's output or handling potential API errors?
*   How would you extend this script to include more advanced features like:
    *   Batching multiple articles into a single API call (if supported by the model).
    *   Implementing exponential backoff for rate limit handling.
    *   Adding a mechanism to store and retrieve prompts from a database instead of hardcoding them?
*   Consider the security implications of handling API keys in a production environment.

## Best Practices for Programmatic LLM Integration

*   **Modularize Prompts:** Store prompt templates separately from your application logic (e.g., in `.txt` or `.jinja` files).
*   **Use SDKs:** Leverage official LLM SDKs for ease of use, built-in features, and adherence to API best practices.
*   **Structured Outputs:** Whenever possible, instruct the LLM to generate structured outputs (JSON, XML) and use robust parsing libraries to process them.
*   **Error Handling & Retries:** Implement comprehensive error handling, including retry mechanisms with exponential backoff for transient network or rate limit errors.
*   **Asynchronous Calls:** For high-throughput applications, use asynchronous API calls to prevent blocking.
*   **Input Sanitization:** Sanitize user inputs before injecting them into prompts to prevent prompt injection attacks.
*   **Output Validation:** Validate the LLM's output against expected schemas or rules to ensure correctness and safety.
*   **Logging and Monitoring:** Log API requests, responses, and errors. Monitor token usage and latency in production.
*   **Cost Optimization:** Be mindful of token usage. Experiment with different models (smaller models for simpler tasks) and optimize prompt length.
*   **Version Control:** Treat your prompts and prompt templates as code and manage them in version control.
