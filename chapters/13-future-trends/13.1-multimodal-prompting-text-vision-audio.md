# 13.1 Multimodal Prompting (Text, Vision, Audio)

Future LLMs will natively handle and integrate multiple modalities—text, images, and audio—enabling richer interactions.

## Key Concepts

- **Cross-Modal Embeddings:** Joint vector spaces for text, images, and audio.  
- **Prompt Formats:** Combine instructions across modalities (e.g., “Describe this image in three sentences”).  
- **Alignment Challenges:** Ensuring semantic coherence when translating between modalities.  
- **Applications:** Visual question answering, image-aware chatbots, audio transcription with context.

## Example Prompt

```
You are a museum guide. Analyze this image of an ancient artifact:
![artifact.jpg]
Describe its era, probable use, and stylistic features in bullet points.
```

## Hands-On Exercise

1. In a multimodal playground (e.g., OpenAI Vision), upload an image.  
2. Prompt for a description, then for follow-up questions about colors and style.  
3. Experiment with adding audio context:  
   ```
   You are a travel narrator. Play this audio clip [clip.mp3]. Summarize the sounds and speculate on location.
   ```

## Reflection

- How did combining modalities affect prompt complexity?  
- What ambiguities arose in cross-modal descriptions?  
- Which applications will benefit most from multimodal prompting?
