# 7.3 Logging, Monitoring & Debugging Tools

Effective logging and monitoring help you track prompt performance, detect regressions, and diagnose errors in production.

## Key Concepts

- **Logging:** Capture inputs, outputs, metadata (timestamps, model parameters).  
- **Monitoring:** Aggregate metrics (latency, error rates, success rates) over time.  
- **Alerting:** Notify when key performance indicators (KPIs) cross thresholds.  
- **Debugging:** Replay inputs, inspect logs, and reproduce failures.

## Tooling Examples

- **LangKit Logger (hypothetical):**  
  ```python
  from langkit import PromptLogger
  logger = PromptLogger(storage="file://logs/prompts.json")
  response = llm(prompt="â€¦")
  logger.log(prompt=prompt, response=response)
  ```
- **OpenTelemetry + Prometheus:**  
  - Instrument API calls to record latency and error counts.  
  - Export metrics to Prometheus and visualize in Grafana dashboards.  
- **Sentry for Exceptions:**  
  - Wrap prompt execution in a try/catch and send exceptions to Sentry for tracebacks.

## Hands-On Exercise

1. Instrument a simple script that sends prompts via the OpenAI SDK:
   - Log each prompt and response to a JSON or database.  
   - Include metadata: model name, temperature, and timestamp.
2. Set up Prometheus to scrape a metrics endpoint exposing:
   - Total prompts served  
   - Average response time  
   - Error count  
3. Create a Grafana dashboard to visualize trends and set an alert for response time > 1s.

## Reflection

- Which metrics provided the clearest signal of performance drift?  
- How quickly could you identify and reproduce a failing prompt?  
- What logging granularity balanced insight vs. storage cost?
