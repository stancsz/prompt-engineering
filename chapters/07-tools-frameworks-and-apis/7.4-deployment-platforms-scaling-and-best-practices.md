# 7.4 Deployment Platforms, Scaling & Best Practices

Putting your prompt pipelines into production requires robust infrastructure and careful design.

## Platforms & Services

- **Managed APIs (OpenAI, Anthropic, Azure OpenAI)**  
  - Pros: Automatic scaling, uptime SLAs, usage analytics.  
  - Cons: Cost per call, vendor lock-in.  
- **Self-Hosted Models (Llama 2, Mistral, local GPT-J)**  
  - Pros: Full control, no per-call fees.  
  - Cons: Requires GPU/CPU provisioning, maintenance.  
- **Hybrid Architecture**  
  - On-premises for sensitive data, cloud for burst capacity.

## Scaling Strategies

1. **Batching & Asynchronous Calls**  
   - Group prompts into a single request where supported.  
   - Use async I/O (e.g., Python `asyncio`, Node.js promises).  
2. **Caching Frequent Queries**  
   - Store prompt→response mappings in Redis or in-memory cache.  
   - Invalidate cache on template changes.  
3. **Rate Limit Management**  
   - Throttle requests to avoid 429 errors.  
   - Exponential backoff and retry policies.

## Best Practices

- **Prompt Versioning**  
  - Tag prompts with identifiers (e.g., “v1.2-summary”).  
  - Keep a git-backed registry or database of prompt templates.  
- **Environment Separation**  
  - Use staging vs. production keys and endpoints.  
  - Mirror traffic patterns before upgrades.  
- **Monitoring & Alerts**  
  - Track latency, error rates, token consumption.  
  - Alert on sudden spikes or degraded accuracy.

## Example (Async Batching in Python)

```python
import asyncio
from openai import OpenAI

client = OpenAI()

async def batch_prompts(prompts):
    tasks = [client.chat.completions.acreate(
        model="gpt-4", messages=[{"role":"user","content":p}]
    ) for p in prompts]
    return await asyncio.gather(*tasks)

prompts = ["Summarize A","Summarize B","Summarize C"]
responses = asyncio.run(batch_prompts(prompts))
```

## Hands-On Exercise

1. Deploy a simple Flask or FastAPI service that wraps an LLM call.  
2. Implement caching with Redis: cache responses for identical prompts for 24 hours.  
3. Simulate load with `locust` or `k6`, monitor latency and error rate.

## Reflection

- How did batching affect throughput and cost?  
- What caching eviction policy balanced freshness vs. hit rate?  
- Which metrics were most critical to production stability?
