# 7.2 Notebook-Based Experimentation (Jupyter, Colab, VS Code Notebooks)

Interactive computing environments, particularly **Jupyter Notebooks** (including JupyterLab, Google Colab, and VS Code Notebooks), serve as the primary workbench for prompt engineers. They provide an ideal environment for rapid experimentation, iterative refinement, visualization of outputs, and collaborative development of prompts and LLM-powered workflows.

## Why Notebooks are Essential for Prompt Engineering

1.  **Interactivity and Rapid Iteration:**
    *   Execute code cell by cell, allowing immediate feedback on prompt changes.
    *   Modify prompts, parameters, or input data on the fly and re-run cells instantly.
    *   Facilitates a "try-and-see" approach crucial for prompt tuning.
2.  **Visualization and Exploration:**
    *   Embed charts, tables, and even rich HTML outputs directly alongside code.
    *   Visualize prompt outputs, compare responses from different models or parameters, and analyze data distributions.
    *   Useful for plotting evaluation metrics (Chapter 6.1) or human ratings (Chapter 6.2).
3.  **Documentation and Reproducibility:**
    *   Combine code, markdown text (for explanations), and outputs in a single document.
    *   Serves as a living record of your experiments, hypotheses, and findings.
    *   While not a production environment, well-documented notebooks can aid reproducibility of experiments.
4.  **Shareability and Collaboration:**
    *   Notebook files (`.ipynb`) are easily shareable.
    *   Platforms like Google Colab enable real-time collaborative editing.
    *   Tools like GitHub's `nbviewer` allow rendering notebooks directly in web browsers.
5.  **Prototyping Complex Workflows:**
    *   Ideal for prototyping prompt chains, RAG pipelines, or agentic behaviors before porting them to production code.

## Example Setup: Your First Prompt in a Notebook

1.  **Create a New Notebook:** Open JupyterLab, Google Colab, or a new VS Code Notebook.
2.  **Install Necessary SDKs:** Run the following in a notebook cell (using `!` for shell commands):
    ```bash
    !pip install openai langchain-openai pandas matplotlib seaborn
    ```
3.  **Initialize LLM Client and Test a Basic Prompt:**
    ```python
    import os
    from openai import OpenAI
    from langchain_openai import ChatOpenAI
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns

    # Ensure your OpenAI API key is set as an environment variable
    # os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY" # Uncomment and replace for direct testing

    # Initialize OpenAI client
    client = OpenAI()

    # Initialize LangChain LLM (optional, but good for consistency with later examples)
    llm_langchain = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.7)

    # Test a simple prompt
    prompt_text = "Explain the concept of prompt engineering in one concise sentence."
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt_text}]
    )
    print("OpenAI SDK Response:")
    print(response.choices[0].message.content)

    # Test with LangChain
    langchain_response = llm_langchain.invoke(prompt_text)
    print("\nLangChain Response:")
    print(langchain_response.content)
    ```

## Hands-On Exercise: Comparing Prompt Variants in a Notebook

This exercise demonstrates how to systematically compare different prompt variants and visualize their outputs.

1.  **Define a Wrapper Function for LLM Calls:**
    ```python
    def get_llm_response_with_params(prompt_content, model="gpt-3.5-turbo", temperature=0.7, max_tokens=100):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt_content}],
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"Error: {e}"
    ```
2.  **Create a DataFrame of Prompt Variants:**
    ```python
    # Scenario: Summarizing a product review for different audiences/tones
    review_text = "The new XYZ speaker has amazing sound quality, but the battery life is disappointingly short, only lasting about 4 hours."

    prompt_variants = [
        {
            "name": "Concise Summary",
            "prompt_template": "Summarize the following review concisely: \"\"\"{review}\"\"\"",
            "temperature": 0.3
        },
        {
            "name": "Marketing Summary",
            "prompt_template": "You are a marketing expert. Write a positive summary of the following review, highlighting strengths and downplaying weaknesses: \"\"\"{review}\"\"\"",
            "temperature": 0.7
        },
        {
            "name": "Technical Summary",
            "prompt_template": "Summarize the technical aspects of the following review, focusing on performance metrics: \"\"\"{review}\"\"\"",
            "temperature": 0.2
        }
    ]

    results_df = pd.DataFrame(prompt_variants)
    results_df['input_review'] = review_text
    ```
3.  **Generate Outputs and Store in DataFrame:**
    ```python
    # Apply the LLM call function to each prompt variant
    results_df['generated_output'] = results_df.apply(
        lambda row: get_llm_response_with_params(
            prompt_content=row['prompt_template'].format(review=row['input_review']),
            temperature=row['temperature']
        ),
        axis=1
    )

    # Display results side-by-side
    pd.set_option('display.max_colwidth', None) # Display full content
    print(results_df[['name', 'prompt_template', 'generated_output']])
    ```
4.  **Add Simple Evaluation (e.g., Length):**
    ```python
    results_df['output_length_tokens'] = results_df['generated_output'].apply(
        lambda x: len(client.encodings.cl100k_base().encode(x)) # Using OpenAI's tokenizer for estimation
    )
    print("\nResults with Output Length:")
    print(results_df[['name', 'output_length_tokens', 'generated_output']])
    ```

## Visualization: Analyzing Prompt Impact

Notebooks excel at visualizing data. You can plot relationships between prompt parameters and output characteristics.

```python
# Example: Plotting output length vs. temperature (conceptual data)
# Assuming results_df has 'temperature' and 'output_length_tokens' columns
# For a real plot, you'd run more experiments with varying temperatures.

# Create some dummy data for demonstration if your results_df is small
if len(results_df) < 5:
    dummy_data = {
        'temperature': [0.1, 0.5, 0.9, 1.2, 1.5],
        'output_length_tokens': [50, 70, 90, 110, 130],
        'name': ['T0.1', 'T0.5', 'T0.9', 'T1.2', 'T1.5']
    }
    dummy_df = pd.DataFrame(dummy_data)
    # Combine with actual results for a richer plot if desired
    plot_df = pd.concat([results_df[['temperature', 'output_length_tokens', 'name']], dummy_df])
else:
    plot_df = results_df[['temperature', 'output_length_tokens', 'name']]


plt.figure(figsize=(8, 5))
sns.scatterplot(data=plot_df, x='temperature', y='output_length_tokens', hue='name', s=100)
plt.title('Output Length vs. Temperature')
plt.xlabel('Temperature')
plt.ylabel('Output Length (Tokens)')
plt.grid(True)
plt.show()
```

## Advanced Notebook Techniques

*   **Widgets:** Use `ipywidgets` to create interactive sliders or dropdowns for prompt parameters, allowing non-coders to experiment.
*   **Logging Integrations:** Integrate with MLOps platforms like Weights & Biases Prompts or MLflow to log prompt inputs, outputs, and metrics automatically.
*   **Version Control:** Use Git for versioning your notebooks. Be mindful of large outputs in `.ipynb` files.
*   **Parameter Sweeps:** Programmatically run experiments across a grid of prompt parameters and collect results.

## Reflection

*   How did the interactive nature of the notebook accelerate your prompt tuning process compared to editing a script and re-running it?
*   Which aspects of the generated outputs were most effectively compared side-by-side in the DataFrame?
*   What kind of visualizations would be most helpful for your specific prompt engineering tasks (e.g., comparing accuracy across different few-shot examples, analyzing response diversity)?
*   How can you ensure that your notebook experiments are reproducible and easily shareable with colleagues?

## Best Practices for Notebook-Based Prompt Engineering

*   **Organize Cells Logically:** Group imports, setup, prompt definitions, and execution cells.
*   **Document Thoroughly:** Use Markdown cells to explain your hypotheses, experiments, and observations.
*   **Clear Outputs:** Ensure outputs are easy to read and interpret. Use `print()` statements or `display()` for clarity.
*   **Manage API Keys Securely:** Never hardcode API keys directly in your notebook. Use environment variables.
*   **Version Control:** Commit your notebooks regularly. Consider tools that strip outputs before committing if file size is an issue.
*   **Modularize Code:** As experiments grow, refactor reusable functions into separate Python files and import them into your notebook.
*   **Transition to Production:** Notebooks are for experimentation. Production code should typically be in `.py` scripts, often leveraging the same SDKs and frameworks prototyped in the notebook.
*   **Clean Up:** Clear outputs before saving and sharing to reduce file size.
