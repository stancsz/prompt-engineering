# Appendix B: Glossary of Terms & Acronyms

A quick reference for essential terminology in prompt engineering.

-   **LLM** (Large Language Model): An AI model trained on vast text data to understand and generate human-like language.
-   **Prompt**: The input text or instruction given to an LLM to guide its output.
-   **Token**: A basic unit of text (word or subword) that LLMs process.
-   **Zero-Shot Learning**: A method where the model performs a task without any prior examples in the prompt.
-   **Few-Shot Learning**: A method where the model is given a small number of examples in the prompt to learn the task.
-   **Chain-of-Thought (CoT)**: A prompting technique that encourages the LLM to show its step-by-step reasoning process.
-   **RAG** (Retrieval-Augmented Generation): A technique that combines an LLM with an information retrieval system to provide relevant context before generating a response.
-   **Temperature**: A parameter that controls the randomness or creativity of the LLM's output. Higher values mean more diverse outputs.
-   **Top-k / Top-p Sampling**: Decoding methods that limit the selection of the next token based on its probability rank (Top-k) or cumulative probability (Top-p).
-   **Hallucination**: When an LLM generates factually incorrect or fabricated information that sounds plausible.
-   **Prompt Injection**: A security vulnerability where malicious input in a prompt overrides the system's original instructions.
-   **Guardrails**: Mechanisms or rules implemented to ensure an LLM's output remains safe, ethical, and aligned with desired policies.
-   **Embedding**: A numerical vector representation of text that captures its semantic meaning, allowing for mathematical comparisons.
-   **Fine-tuning**: The process of further training a pre-trained LLM on a smaller, specific dataset to adapt it to a particular task or domain.
-   **Instruction Tuning**: A type of fine-tuning where models are trained on datasets of instructions and desired outputs to improve their ability to follow commands.
-   **Context Window**: The maximum amount of text (in tokens) that an LLM can process or "remember" at one time.
-   **Meta-Prompting**: Using an LLM to generate, evaluate, or refine other prompts.
-   **Automated Evaluation**: Using metrics (e.g., BLEU, ROUGE) or automated systems to assess the quality of LLM outputs.
-   **Human-in-the-Loop (HITL)**: A system design where human oversight and intervention are integrated into an AI workflow for quality control or ethical review.
-   **MLOps**: A set of practices for deploying and maintaining machine learning models in production, including monitoring and versioning.
-   **Adversarial Prompting**: Crafting prompts to intentionally elicit undesirable or harmful responses from an LLM, often for testing security.
-   **Bias**: Systematic errors or unfairness in an LLM's output, often stemming from biases in its training data.
-   **Decoding Strategy**: The method used by an LLM to select the next token in a sequence during text generation.
-   **Prompt Engineering Lifecycle**: The iterative process of designing, testing, evaluating, and refining prompts for AI applications.
-   **Vector Database**: A specialized database designed to store and query high-dimensional vectors (embeddings), often used in RAG systems.


# Combined Glossary

**.ipynb**: The standard file extension for Jupyter Notebooks.
**A/B testing**: (also known as split testing or controlled experiments) provides a scientific framework, allowing you to compare two or more prompt variants under controlled conditions to identify the best performer with statistical confidence.
**Abstractive Summarization**: Generating new sentences and phrases that capture the main ideas, potentially rephrasing content.
**Access Control and Permissions**: Defining who has permission to read, create, modify, approve, or retire prompts within the library.
**Accuracy/Factuality**: Is the information presented factually correct and free from hallucinations? (Crucial for factual tasks).
**Agents (LangChain)**: LLMs within LangChain that can use tools (e.g., search engines, calculators, custom APIs) to achieve a goal.
**Alerting**: Automatically notifying relevant teams or individuals when monitored metrics cross predefined thresholds or when critical events occurs.
**Alternative Hypothesis (H1)**: States that there *is* a significant difference between the groups, and the treatment group is indeed better (or worse) than the control.
**APM (Application Performance Monitoring)**: Tools that combine metrics, logs, and traces to monitor the performance and health of applications.
**Arize AI**: An MLOps platform that extends to LLM observability, offering experiment tracking, prompt versioning, and performance monitoring.
**Asynchronous Calls**: Using non-blocking API calls to send multiple requests concurrently, improving overall responsiveness.
**Attention mechanisms**: allow an LLM to weigh the importance of different parts of the input sequence when processing each token.
**Auto-Scaling**: Automatically adjusting the number of running instances based on demand to ensure optimal resource utilization and performance.
**automated metrics**: quantitative ways to measure the performance of LLM outputs, compare different prompt versions, and track improvements over time.
**autoregressive**: meaning they generate text one token at a time, predicting the next token based on all previously generated tokens and the input prompt.
**Batching**: Grouping multiple independent prompts into a single API request to reduce overhead and improve throughput.
**Beam search**: a greedy decoding algorithm that explores multiple possible sequences (called "beams") simultaneously to find the most probable output.
**BERT (Bidirectional Encoder Representations from Transformers) - Encoder-Only Models**: These models consist solely of a Transformer encoder stack. They are trained to understand context from both left and right sides of a word in a sentence.
**Binary Judgments (Yes/No)**: For objective criteria or quick filtering.
**BLEU (Bilingual Evaluation Understudy)**: a precision-focused metric primarily used for evaluating machine translation. It measures the n-gram overlap between the generated text (hypothesis) and one or more human-written reference translations.
**Byte-Pair Encoding (BPE)**: Iteratively merges the most frequent pairs of characters or subwords.
**Caching**: Storing responses to common or identical prompts in a fast-access cache to reduce API calls, lower latency, and decrease load on the LLM.
**Callbacks (LangChain)**: Hooks in LangChain for logging, streaming, and monitoring LLM interactions.
**Canary Deployments**: A deployment strategy where a new version of a service is rolled out to a small subset of users before being deployed to the entire infrastructure.
**Chain-of-Thought (CoT) prompting**: encourages the model to "think aloud" by generating intermediate reasoning steps before providing a final answer.
**Chains (LangChain)**: Sequences of LLM calls or other utilities within the LangChain framework.
**Chunking**: Breaking down large bodies of text (documents, conversations) into smaller, manageable segments or "chunks" that fit within the LLM's context window.
**CI/CD (Continuous Integration/Continuous Delivery)**: A set of practices that automate the integration and delivery of code changes, including prompt changes, into production.
**clarity**: of your instructions
**Clarity and Specificity**: Prompts should be unambiguous, detailing exactly what is expected from the model.
**CLM (Causal Language Modeling)**: (for GPT-like models) predicting the next word.
**Coherence/Consistency**: Does the response flow logically? Is it internally consistent? Does it maintain consistency with previous turns in a conversation?
**Collaboration**: Adopting established software development practices becomes essential.
**Completeness**: Does the response provide all necessary information to fulfill the request?
**Conciseness**: Is the response brief and to the point, without unnecessary verbosity? (Important for summarization, extraction).
**Confidence Interval**: A range of values within which the true difference between the groups is likely to fall.
**Constraint Definition**: Specifying limitations on length, format, or content helps narrow down the model's output space.
**Context (Semantic Kernel)**: Manages the state and memory for the LLM within Semantic Kernel.
**Context Provision**: Supplying relevant background information helps the model generate more informed and accurate responses.
**context window**: limits the amount of text they can process at once.
**Context Window Limits**: LLMs have a maximum number of tokens they can process in a single input.
**Contextual embeddings**: meaning the embedding for a word like "bank" will differ depending on whether it refers to a financial institution or a river bank.
**Continuous Deployment (CD)**: Automatically publish approved prompt templates to production endpoints or shared libraries.
**Continuous Integration (CI)**: Automatically build and test prompts on each commit.
**Control Group**: The baseline version of the prompt or system that you are currently using or that represents the status quo.
**Cost Optimization**: Strategies to reduce the expenses associated with LLM usage, including monitoring token usage, optimizing prompt length, choosing appropriate model sizes, and implementing caching.
**Creativity/Novelty**: For generative tasks, is the output original, imaginative, and not repetitive?
**Data Connectors (LlamaIndex)**: Tools in LlamaIndex to load data from various sources.
**Datadog**: A monitoring and security platform for cloud applications, offering APM, logging, and metrics.
**Debugging**: The process of identifying, isolating, and resolving issues within your LLM application, often leveraging logs and monitoring data.
**decoder**: generates output
**decoding controls**: These parameters govern how the Large Language Model (LLM) selects the next token during the text generation process, allowing you to fine-tune the output's creativity, diversity, coherence, and determinism.
**Deep Learning and Neural Networks**: Introduction of neural sequence models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTMs).
**Delimiters**: Employ clear separators (e.g., triple quotes, XML tags) to distinguish instructions from context or examples.
**Deployment Platforms**: Services and infrastructure used to host and run LLM applications.
**Distillation**: A technique for self-hosted models that involves training a smaller model to mimic a larger one, reducing model size and inference cost.
**Document Loaders (LangChain)**: Tools in LangChain to load data from various sources (PDFs, websites, databases).
**Dynamic Variables (Placeholders)**: Specific sections within the static text that are designed to be replaced with user-provided input, retrieved data, or other context-specific information at the time the prompt is sent to the LLM.
**ELK Stack**: A collection of open-source products—Elasticsearch, Logstash, and Kibana—used for centralized logging, searching, analyzing, and visualizing log data.
**embedding**: After tokenization, each token (or sequence of tokens) is converted into a dense numerical vector called an *embedding*.
**Embedding Similarity**: Measures the semantic similarity between the embedding vector of the generated text and the embedding vector of a reference text (or query).
**Embeddings & Vectorstores (LangChain)**: Integrations in LangChain for Retrieval-Augmented Generation (RAG).
**encoder**: processes input
**ensemble methods**: These techniques involve generating multiple outputs and then aggregating them to arrive at a more robust and often more accurate final answer.
**Ensemble prompts**: involve using *different* prompt formulations, strategies, or even different LLM models to generate outputs for the same task, and then combining or selecting the best result.
**Environment Separation**: Maintaining distinct development, staging (or UAT), and production environments with separate API keys and endpoints.
**ETL (Extract, Transform, Load)**: Data Extraction and Transformation.
**example-based prompt**: (also known as few-shot prompting) provides one or more input-output examples *before* the actual query.
**Experiment Run Tracking**: Recording all relevant information about each "run" or iteration of a prompt experiment.
**Experiment Tracking**: Recording all relevant information about each "run" or iteration of a prompt experiment.
**Exponential Backoff**: A retry strategy that waits progressively longer between retries to gracefully handle rate limit errors and transient network issues.
**Extractive Summarization**: Identifying and extracting key sentences or phrases directly from the original text.
**Few-Shot CoT**: involves providing a few examples of input-output pairs where the output explicitly includes the step-by-step reasoning, followed by the new query.
**Few-Shot Learning**: The prompt includes multiple (typically 2-5, but can be more) examples of input-output pairs that illustrate the task, followed by the actual query.
**Fine-Tuning**: To adapt a pretrained model's general knowledge to a specific downstream task or domain.
**Fixed-Size Chunking**: Splitting text into chunks of a predetermined token count.
**Fluency/Readability**: Is the generated text grammatically correct, natural-sounding, and easy to read?
**frame**: your request
**Frameworks**: Higher-level tools that provide abstractions and built-in implementations for common LLM patterns.
**Git**: A distributed version control system for tracking changes in source code during software development.
**Google Colab**: A cloud-based Jupyter notebook environment provided by Google.
**GPT (Generative Pre-trained Transformer) - Decoder-Only Models**: These models consist solely of a Transformer decoder stack. They are *autoregressive*, meaning they generate text one token at a time, predicting the next token based on all previously generated tokens and the input prompt.
**Graceful Degradation**: Designing an application to handle failures or unavailability of LLMs gracefully, such as by falling back to simpler responses or informing the user.
**Grafana**: An open-source visualization and dashboarding tool, often used with Prometheus for creating dashboards.
**Greedy Decoding**: The simplest decoding strategy, where the model always selects the token with the highest probability at each step.
**Helicone**: An open-source platform for LLM observability, which proxies API calls to track usage, latency, and costs.
**higher-order prompts**: refer to a sequence or chain of prompts where the output of one prompt serves as the input or a refinement for a subsequent prompt.
**human evaluation**: the gold standard for many prompt engineering tasks, especially those involving open-ended generation, complex reasoning, or user-facing applications.
**Hybrid Architecture**: A deployment strategy combining managed APIs and self-hosted models to leverage the strengths of both.
**in-context learning**: The model leverages its vast pre-trained knowledge to infer the task from the provided context.
**Indexes (LlamaIndex)**: Structures in LlamaIndex to organize data for efficient retrieval (e.g., vector indexes, tree indexes).
**Input/Output Sanitization**: Cleaning and validating user inputs before sending them to LLMs and validating LLM outputs to prevent attacks like prompt injection.
**Instruction tuning**: where models were trained to follow instructions.
**Instruction Tuning**: To teach the model to follow human instructions and align its outputs with human preferences, making it more useful and conversational.
**instruction-based prompts**: provides explicit, natural language commands or directives that describe the task the LLM should perform.
**Iterative Refinement**: Prompt engineering is an iterative process. Initial prompts are rarely perfect and require continuous testing and adjustment.
**Iterative Summarization**: For very long conversations, periodically summarizing the conversation history and replacing the raw history with its summary.
**Jupyter Notebooks**: Interactive computing environments (including JupyterLab, Google Colab, and VS Code Notebooks) used for rapid experimentation, iterative refinement, visualization, and collaborative development of prompts and LLM workflows.
**LangSmith**: A dedicated platform by LangChain for debugging, testing, evaluating, and monitoring LLM applications built with LangChain.
**Least-to-Most Prompting**: Break down a complex problem into a series of simpler sub-problems, solve each sub-problem, and then use the solutions as context for the next sub-problem.
**Likert Scales**: (e.g., 1-5): For subjective criteria like fluency, relevance, usefulness.
**LLM (Large Language Model)**: A type of artificial intelligence model trained on vast amounts of text data to understand and generate human-like language.
**LLMs (Large Language Models)**: powerful, but their effectiveness is highly dependent on the quality of the input they receive.
**Load Balancing**: Distributing incoming requests across multiple instances of an application or multiple LLM endpoints to prevent single points of failure or overload.
**Logging**: The practice of recording events, data, and messages generated by your application during its execution, capturing the full lifecycle of a prompt interaction.
**Managed LLM APIs**: Utilizing LLMs as a service directly from cloud providers (e.g., OpenAI API, Anthropic API, Google Cloud Vertex AI, Azure OpenAI Service), where the provider handles model inference, infrastructure, and scaling.
**Memory (LangChain)**: Component in LangChain to manage conversational history.
**Meta-Prompt**: A prompt whose primary purpose is to instruct an LLM to generate or modify another prompt.
**meta-prompting**: involves using an LLM to generate, refine, or evaluate other prompts, enabling more dynamic, scalable, and autonomous workflows.
**Metadata**: Additional information stored alongside each prompt that provides context, usage guidelines, and performance insights.
**Metadata and Tagging**: Attaching descriptive information to runs and prompts to facilitate search, filtering, and analysis.
**Metric of Success**: A quantifiable measure that indicates whether a prompt variant is performing better.
**MLflow**: An open-source platform for managing the end-to-end machine learning lifecycle, including experiment tracking, reproducible runs, and model management.
**MLM (Masked Language Modeling)**: (for BERT-like models) predicting masked words.
**MLOps (Machine Learning Operations)**: Connecting prompt experimentation and management into the broader Machine Learning Operations (MLOps) lifecycle, which encompasses data management, model development, deployment, and monitoring.
**MLOps Integration**: Connecting prompt experimentation and management into the broader Machine Learning Operations (MLOps) lifecycle, which encompasses data management, model development, deployment, and monitoring.
**MLOps Platforms**: Tools like MLflow, Kubeflow, or DVC can help track prompt versions alongside model versions and data.
**Model Optimization**: Techniques like using smaller models, quantization, or distillation to reduce model size and inference cost.
**Monitoring**: The continuous collection and aggregation of metrics over time to track the health, performance, and usage patterns of your LLM application.
**Multi-Head Attention**: Instead of a single attention mechanism, Transformers use multiple "attention heads" in parallel.
**Negative Constraints**: Telling the Model What *Not* to Do
**New Relic**: An Application Performance Monitoring (APM) tool that combines metrics, logs, and traces for comprehensive application observability.
**NLP (Natural Language Processing)**: has undergone a significant transformation, evolving from rudimentary rule-based systems to sophisticated large language models (LLMs).
**Null Hypothesis (H0)**: States that there is no significant difference between the control and treatment groups. Any observed difference is due to random chance.
**Observability**: The ability to understand the internal state of your system from its external outputs, including comprehensive logging, real-time monitoring, and effective debugging tools.
**One-Shot Learning**: The prompt includes a single example of an input-output pair that demonstrates the desired task or format, followed by the actual query.
**OpenTelemetry**: A set of APIs, SDKs, and tools to instrument, generate, collect, and export telemetry data (metrics, logs, traces).
**Overlap Chunking**: Adding a small overlap (e.g., 10-20% of tokens) between consecutive chunks to preserve context across boundaries.
**p-value**: A common measure of statistical significance.
**PEFT (Parameter-Efficient Fine-Tuning)**: Techniques that allow for fine-tuning large models with fewer trainable parameters, reducing computational cost.
**Perplexity (PPL)**: a measure of how well a probability distribution or language model predicts a sample. In simpler terms, it quantifies how "surprised" the model is by a given sequence of words.
**phrasing**: you choose
**PII (Personally Identifiable Information)**: Information that can be used to identify, contact, or locate a single person, or can be directly linked to a person.
**Planner (Semantic Kernel)**: An LLM-powered component in Semantic Kernel that orchestrates calls to skills to achieve a user's goal.
**Pretraining**: To learn general language understanding, generation, and world knowledge from vast amounts of unlabeled text data.
**Prometheus**: An open-source monitoring system that collects metrics from configured targets.
**Prompt engineering**: the discipline of designing, refining, and optimizing the inputs (prompts) provided to large language models (LLMs) to achieve desired, accurate, and contextually relevant outputs.
**Prompt Injection**: A type of attack where malicious input is crafted to manipulate an LLM's behavior or extract sensitive information.
**Prompt Linting**: Automated static analysis of prompt text and structure to enforce coding standards, best practices, and identify potential issues early.
**Prompt Management Platforms**: Platforms for sharing, discovering, and managing LangChain prompts.
**Prompt Quality**: Akin to code quality.
**Prompt Registry**: A dedicated platform or database used to version, track, and deploy prompts.
**Prompt Registry / Library**: A centralized, versioned collection of all prompts, prompt templates, few-shot examples, and associated metadata used across an organization or project. It serves as the single source of truth for all prompt assets.
**Prompt Review Checklist**: A standardized set of criteria that reviewers use to evaluate prompt changes during a pull request.
**Prompt Templates (LangChain)**: Components in LangChain used to manage dynamic prompt construction.
**Prompt Versioning**: Treating prompts as code, storing them in version control, and using a dedicated prompt registry to track and deploy prompts.
**Prompting SDKs (Software Development Kits)**: Provide the necessary tools and abstractions to simplify building scalable, maintainable, and intelligent LLM-powered systems.
**Pull Requests (or Merge Requests) for Prompts**: Any change to a prompt (new prompt, update, deprecation) is proposed via a pull request.
**Quantization**: A technique for self-hosted models that reduces model size and inference cost by reducing the precision of model parameters.
**Query Engines (LlamaIndex)**: Components in LlamaIndex that combine retrieval and LLM generation for Q&A over data.
**RAG (Retrieval Augmented Generation)**: Embeddings are central to RAG systems.
**RAG (Retrieval-Augmented Generation)**: A technique that combines information retrieval with text generation to improve the factual accuracy and relevance of LLM outputs.
**Rate Limit Management**: Strategies to handle limits on the number of requests or tokens per minute/second imposed by LLM APIs, often involving retry logic with exponential backoff.
**Recursive Chunking**: A more advanced method that attempts to split text by larger units (e.g., sections, paragraphs) first, then recursively splits smaller units if they still exceed the limit.
**Relevance**: Does the output directly address the user's query or the prompt's intent? Is it on-topic?
**Repository Management**: Practices is essential for ensuring consistency, discoverability, reusability, and version control of your prompts at scale.
**Review Processes**: And a focus on **prompt quality** akin to code quality.
**RLHF (Reinforcement Learning from Human Feedback)**: Human evaluators rank model outputs, and this feedback is used to further optimize the model's behavior through reinforcement learning, aligning it with human values and instructions.
**Role Assignment**: Defining a persona for the LLM (e.g., "You are an expert historian") can significantly influence the style and content of its output.
**Roles and Responsibilities**: Clearly defined roles for prompt creation, review, and approval within the team.
**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: a recall-focused metric commonly used for evaluating summarization and machine translation. It measures the overlap of n-grams, word sequences, or word pairs between the generated summary and one or more reference summaries.
**Rule-Based and Symbolic Systems**: NLP systems relied on handcrafted rules, lexicons, and grammatical structures.
**Safety/Harmlessness**: Is the content free from bias, toxicity, hate speech, or other harmful outputs? (Critical for all applications).
**Scaled Dot-Product Attention**: A specific, efficient way to compute these attention scores.
**Self-Attention**: The core innovation of the Transformer. Each token in the input sequence computes an "attention score" with every other token.
**Self-Consistency**: Generate multiple CoT paths and then aggregate the final answers (e.g., by majority vote) to improve robustness.
**Self-Hosted Models**: Deploying open-source LLMs on your own infrastructure, either on dedicated servers (on-premises) or on cloud Virtual Machines (VMs) with GPUs.
**Semantic Similarity**: The cosine similarity between two embedding vectors indicates how semantically similar the corresponding texts are.
**Sentence/Paragraph Chunking**: Splitting text based on natural linguistic boundaries (sentences, paragraphs).
**SentencePiece**: Used by T5, handles multiple languages and allows for training a tokenizer directly from raw text.
**Sentry**: A real-time error monitoring tool that provides detailed stack traces and context for exceptions.
**Shadow Mode**: A deployment strategy where a new version of a service runs alongside the old version, processing live traffic but not affecting the live responses, used for testing.
**Skills/Plugins (Semantic Kernel)**: Collections of functions (native code or prompts) that the LLM can call within Semantic Kernel.
**Splunk**: A software platform used to search, analyze, and visualize machine-generated data.
**static templates and dynamic variables**: provides a fixed prompt structure with placeholders that are populated at runtime.
**Static Text**: The unchanging core instructions, context, or formatting guidelines that define the general task.
**Statistical NLP**: Shifted to machine learning models trained on large text corpora.
**Statistical Significance**: The probability that the observed difference between the control and treatment groups is not due to random chance, but rather a true effect of the change.
**Structured Logging**: Logging data in a consistent, machine-readable format (e.g., JSON) for easier parsing and analysis by log management systems.
**Summarization**: Condensing previous interactions or long documents into a shorter, more concise summary that can be fed back into the context window.
**T5 / Seq2Seq (Encoder-Decoder Models)**: These models utilize both a Transformer encoder and a decoder. They frame all NLP tasks as a "text-to-text" problem, where both input and output are text sequences.
**Tagging and Categorization**: Organizing prompts using a structured hierarchy and descriptive tags to improve searchability and management.
**Temperature**: a parameter that controls the randomness or "creativity" of the model's output.
**Testing Suite**: Unit tests for prompt rendering, end-to-end tests against a sandbox LLM.
**Text Splitters (LangChain)**: Utilities in LangChain to chunk documents for context management.
**Tokenization**: the process of converting raw text into smaller, discrete units called *tokens*.
**tokens**: the fundamental building blocks that LLMs operate on.
**Tone/Style Adherence**: Does the output match the requested tone (e.g., formal, friendly, witty) or style (e.g., journalistic, poetic)?
**Top-k sampling**: restricts the model's choice for the next token to only the `k` most probable tokens in the vocabulary.
**Top-p sampling (also known as nucleus sampling)**: selects the smallest set of tokens whose cumulative probability exceeds a threshold `p`.
**Tracing**: The process of following the execution path of a single request across multiple services and LLM calls to understand its flow and identify bottlenecks.
**Transformer**: architecture, introduced in 2017, revolutionized NLP by replacing recurrent and convolutional layers with attention mechanisms.
**Transformer Architecture**: (e.g., BERT, GPT) revolutionized NLP. Its self-attention mechanism allowed for parallel processing of sequences and superior capture of long-range dependencies.
**Treating Prompts as Code (Version Control)**: Manage prompt templates and configurations in a version control system (e.g., Git) alongside your application code.
**Treatment Group(s)**: The new variant(s) of the prompt or system that you are testing. You can have multiple treatment groups (A/B/n testing).
**Tree of Thought (ToT)**: A more advanced technique where the model explores multiple reasoning paths in a tree-like structure, allowing for backtracking and more systematic exploration of solutions.
**UAT (User Acceptance Testing)**: A phase of software testing in which the software is tested by the intended audience or users to ensure it meets their needs.
**Usefulness/Helpfulness**: Would a human user find this output genuinely helpful or actionable in a real-world scenario?
**vector database (or vector store)**: Your external knowledge base (documents, databases) is processed and converted into numerical embeddings (vector representations) and stored in a **vector database** (or vector store).
**Vector space**: The collection of all possible embeddings forms a high-dimensional *vector space*.
**Versioned Releases**: Tag stable prompt sets (e.g., `v1.0`) and maintain changelogs.
**Versioning**: Tracking changes to prompts over time, allowing for rollbacks to previous versions, A/B testing of new versions, and clear understanding of what prompt is deployed where.
**Visualization and Dashboards**: Using interactive dashboards to compare experiment runs, visualize performance trends, and identify optimal prompt configurations.
**Voting**: a simpler ensemble technique primarily used for classification, categorization, or selection tasks.
**VS Code Notebooks**: Jupyter notebook integration within Visual Studio Code.
**Weights & Biases (W&B)**: A popular MLOps platform for experiment tracking, visualization, and collaboration.
**Weights & Biases Prompts**: An MLOps platform specifically designed for logging, tracing, and monitoring prompt inputs, outputs, and metrics.
**Widgets (ipywidgets)**: Interactive elements in Jupyter Notebooks used to create interactive controls like sliders or dropdowns for prompt parameters.
**WordPiece**: Used by BERT, similar to BPE but based on likelihood.
**ZenML / Kedro (Orchestration Frameworks)**: MLOps frameworks that help build and orchestrate end-to-end machine learning pipelines.
**Zero-Shot CoT**: where you merely append a phrase like "Let's think step by step" or "Think step by step and then provide the answer" to your original prompt.
**Zero-Shot Learning**: The model performs a task solely based on the natural language instruction provided in the prompt, without any explicit examples of input-output pairs for that specific task.


# Combined Glossary

**.ipynb**: The standard file extension for Jupyter Notebooks.
**A/B testing**: (also known as split testing or controlled experiments) provides a scientific framework, allowing you to compare two or more prompt variants under controlled conditions to identify the best performer with statistical confidence.
**Abstractive Summarization**: Generating new sentences and phrases that capture the main ideas, potentially rephrasing content.
**Access Control and Permissions**: Defining who has permission to read, create, modify, approve, or retire prompts within the library.
**Accuracy/Factuality**: Is the information presented factually correct and free from hallucinations? (Crucial for factual tasks).
**Agents (LangChain)**: LLMs within LangChain that can use tools (e.g., search engines, calculators, custom APIs) to achieve a goal.
**Alerting**: Automatically notifying relevant teams or individuals when monitored metrics cross predefined thresholds or when critical events occurs.
**Alternative Hypothesis (H1)**: States that there *is* a significant difference between the groups, and the treatment group is indeed better (or worse) than the control.
**APM (Application Performance Monitoring)**: Tools that combine metrics, logs, and traces to monitor the performance and health of applications.
**Arize AI**: An MLOps platform that extends to LLM observability, offering experiment tracking, prompt versioning, and performance monitoring.
**Asynchronous Calls**: Using non-blocking API calls to send multiple requests concurrently, improving overall responsiveness.
**Attention mechanisms**: allow an LLM to weigh the importance of different parts of the input sequence when processing each token.
**Auto-Scaling**: Automatically adjusting the number of running instances based on demand to ensure optimal resource utilization and performance.
**automated metrics**: quantitative ways to measure the performance of LLM outputs, compare different prompt versions, and track improvements over time.
**autoregressive**: meaning they generate text one token at a time, predicting the next token based on all previously generated tokens and the input prompt.
**Batching**: Grouping multiple independent prompts into a single API request to reduce overhead and improve throughput.
**Beam search**: a greedy decoding algorithm that explores multiple possible sequences (called "beams") simultaneously to find the most probable output.
**BERT (Bidirectional Encoder Representations from Transformers) - Encoder-Only Models**: These models consist solely of a Transformer encoder stack. They are trained to understand context from both left and right sides of a word in a sentence.
**Binary Judgments (Yes/No)**: For objective criteria or quick filtering.
**BLEU (Bilingual Evaluation Understudy)**: a precision-focused metric primarily used for evaluating machine translation. It measures the n-gram overlap between the generated text (hypothesis) and one or more human-written reference translations.
**Byte-Pair Encoding (BPE)**: Iteratively merges the most frequent pairs of characters or subwords.
**Caching**: Storing responses to common or identical prompts in a fast-access cache to reduce API calls, lower latency, and decrease load on the LLM.
**Callbacks (LangChain)**: Hooks in LangChain for logging, streaming, and monitoring LLM interactions.
**Canary Deployments**: A deployment strategy where a new version of a service is rolled out to a small subset of users before being deployed to the entire infrastructure.
**Chain-of-Thought (CoT) prompting**: encourages the model to "think aloud" by generating intermediate reasoning steps before providing a final answer.
**Chains (LangChain)**: Sequences of LLM calls or other utilities within the LangChain framework.
**Chunking**: Breaking down large bodies of text (documents, conversations) into smaller, manageable segments or "chunks" that fit within the LLM's context window.
**CI/CD (Continuous Integration/Continuous Delivery)**: A set of practices that automate the integration and delivery of code changes, including prompt changes, into production.
**clarity**: of your instructions
**Clarity and Specificity**: Prompts should be unambiguous, detailing exactly what is expected from the model.
**CLM (Causal Language Modeling)**: (for GPT-like models) predicting the next word.
**Coherence/Consistency**: Does the response flow logically? Is it internally consistent? Does it maintain consistency with previous turns in a conversation?
**Collaboration**: Adopting established software development practices becomes essential.
**Completeness**: Does the response provide all necessary information to fulfill the request?
**Conciseness**: Is the response brief and to the point, without unnecessary verbosity? (Important for summarization, extraction).
**Confidence Interval**: A range of values within which the true difference between the groups is likely to fall.
**Constraint Definition**: Specifying limitations on length, format, or content helps narrow down the model's output space.
**Context (Semantic Kernel)**: Manages the state and memory for the LLM within Semantic Kernel.
**Context Provision**: Supplying relevant background information helps the model generate more informed and accurate responses.
**context window**: limits the amount of text they can process at once.
**Context Window Limits**: LLMs have a maximum number of tokens they can process in a single input.
**Contextual embeddings**: meaning the embedding for a word like "bank" will differ depending on whether it refers to a financial institution or a river bank.
**Continuous Deployment (CD)**: Automatically publish approved prompt templates to production endpoints or shared libraries.
**Continuous Integration (CI)**: Automatically build and test prompts on each commit.
**Control Group**: The baseline version of the prompt or system that you are currently using or that represents the status quo.
**Cost Optimization**: Strategies to reduce the expenses associated with LLM usage, including monitoring token usage, optimizing prompt length, choosing appropriate model sizes, and implementing caching.
**Creativity/Novelty**: For generative tasks, is the output original, imaginative, and not repetitive?
**Data Connectors (LlamaIndex)**: Tools in LlamaIndex to load data from various sources.
**Datadog**: A monitoring and security platform for cloud applications, offering APM, logging, and metrics.
**Debugging**: The process of identifying, isolating, and resolving issues within your LLM application, often leveraging logs and monitoring data.
**decoder**: generates output
**decoding controls**: These parameters govern how the Large Language Model (LLM) selects the next token during the text generation process, allowing you to fine-tune the output's creativity, diversity, coherence, and determinism.
**Deep Learning and Neural Networks**: Introduction of neural sequence models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTMs).
**Delimiters**: Employ clear separators (e.g., triple quotes, XML tags) to distinguish instructions from context or examples.
**Deployment Platforms**: Services and infrastructure used to host and run LLM applications.
**Distillation**: A technique for self-hosted models that involves training a smaller model to mimic a larger one, reducing model size and inference cost.
**Document Loaders (LangChain)**: Tools in LangChain to load data from various sources (PDFs, websites, databases).
**Dynamic Variables (Placeholders)**: Specific sections within the static text that are designed to be replaced with user-provided input, retrieved data, or other context-specific information at the time the prompt is sent to the LLM.
**ELK Stack**: A collection of open-source products—Elasticsearch, Logstash, and Kibana—used for centralized logging, searching, analyzing, and visualizing log data.
**embedding**: After tokenization, each token (or sequence of tokens) is converted into a dense numerical vector called an *embedding*.
**Embedding Similarity**: Measures the semantic similarity between the embedding vector of the generated text and the embedding vector of a reference text (or query).
**Embeddings & Vectorstores (LangChain)**: Integrations in LangChain for Retrieval-Augmented Generation (RAG).
**encoder**: processes input
**ensemble methods**: These techniques involve generating multiple outputs and then aggregating them to arrive at a more robust and often more accurate final answer.
**Ensemble prompts**: involve using *different* prompt formulations, strategies, or even different LLM models to generate outputs for the same task, and then combining or selecting the best result.
**Environment Separation**: Maintaining distinct development, staging (or UAT), and production environments with separate API keys and endpoints.
**ETL (Extract, Transform, Load)**: Data Extraction and Transformation.
**example-based prompt**: (also known as few-shot prompting) provides one or more input-output examples *before* the actual query.
**Experiment Run Tracking**: Recording all relevant information about each "run" or iteration of a prompt experiment.
**Experiment Tracking**: Recording all relevant information about each "run" or iteration of a prompt experiment.
**Exponential Backoff**: A retry strategy that waits progressively longer between retries to gracefully handle rate limit errors and transient network issues.
**Extractive Summarization**: Identifying and extracting key sentences or phrases directly from the original text.
**Few-Shot CoT**: involves providing a few examples of input-output pairs where the output explicitly includes the step-by-step reasoning, followed by the new query.
**Few-Shot Learning**: The prompt includes multiple (typically 2-5, but can be more) examples of input-output pairs that illustrate the task, followed by the actual query.
**Fine-Tuning**: To adapt a pretrained model's general knowledge to a specific downstream task or domain.
**Fixed-Size Chunking**: Splitting text into chunks of a predetermined token count.
**Fluency/Readability**: Is the generated text grammatically correct, natural-sounding, and easy to read?
**frame**: your request
**Frameworks**: Higher-level tools that provide abstractions and built-in implementations for common LLM patterns.
**Git**: A distributed version control system for tracking changes in source code during software development.
**Google Colab**: A cloud-based Jupyter notebook environment provided by Google.
**GPT (Generative Pre-trained Transformer) - Decoder-Only Models**: These models consist solely of a Transformer decoder stack. They are *autoregressive*, meaning they generate text one token at a time, predicting the next token based on all previously generated tokens and the input prompt.
**Graceful Degradation**: Designing an application to handle failures or unavailability of LLMs gracefully, such as by falling back to simpler responses or informing the user.
**Grafana**: An open-source visualization and dashboarding tool, often used with Prometheus for creating dashboards.
**Greedy Decoding**: The simplest decoding strategy, where the model always selects the token with the highest probability at each step.
**Helicone**: An open-source platform for LLM observability, which proxies API calls to track usage, latency, and costs.
**higher-order prompts**: refer to a sequence or chain of prompts where the output of one prompt serves as the input or a refinement for a subsequent prompt.
**human evaluation**: the gold standard for many prompt engineering tasks, especially those involving open-ended generation, complex reasoning, or user-facing applications.
**Hybrid Architecture**: A deployment strategy combining managed APIs and self-hosted models to leverage the strengths of both.
**in-context learning**: The model leverages its vast pre-trained knowledge to infer the task from the provided context.
**Indexes (LlamaIndex)**: Structures in LlamaIndex to organize data for efficient retrieval (e.g., vector indexes, tree indexes).
**Input/Output Sanitization**: Cleaning and validating user inputs before sending them to LLMs and validating LLM outputs to prevent attacks like prompt injection.
**Instruction tuning**: where models were trained to follow instructions.
**Instruction Tuning**: To teach the model to follow human instructions and align its outputs with human preferences, making it more useful and conversational.
**instruction-based prompts**: provides explicit, natural language commands or directives that describe the task the LLM should perform.
**Iterative Refinement**: Prompt engineering is an iterative process. Initial prompts are rarely perfect and require continuous testing and adjustment.
**Iterative Summarization**: For very long conversations, periodically summarizing the conversation history and replacing the raw history with its summary.
**Jupyter Notebooks**: Interactive computing environments (including JupyterLab, Google Colab, and VS Code Notebooks) used for rapid experimentation, iterative refinement, visualization, and collaborative development of prompts and LLM workflows.
**LangSmith**: A dedicated platform by LangChain for debugging, testing, evaluating, and monitoring LLM applications built with LangChain.
**Least-to-Most Prompting**: Break down a complex problem into a series of simpler sub-problems, solve each sub-problem, and then use the solutions as context for the next sub-problem.
**Likert Scales**: (e.g., 1-5): For subjective criteria like fluency, relevance, usefulness.
**LLM (Large Language Model)**: A type of artificial intelligence model trained on vast amounts of text data to understand and generate human-like language.
**LLMs (Large Language Models)**: powerful, but their effectiveness is highly dependent on the quality of the input they receive.
**Load Balancing**: Distributing incoming requests across multiple instances of an application or multiple LLM endpoints to prevent single points of failure or overload.
**Logging**: The practice of recording events, data, and messages generated by your application during its execution, capturing the full lifecycle of a prompt interaction.
**Managed LLM APIs**: Utilizing LLMs as a service directly from cloud providers (e.g., OpenAI API, Anthropic API, Google Cloud Vertex AI, Azure OpenAI Service), where the provider handles model inference, infrastructure, and scaling.
**Memory (LangChain)**: Component in LangChain to manage conversational history.
**Meta-Prompt**: A prompt whose primary purpose is to instruct an LLM to generate or modify another prompt.
**meta-prompting**: involves using an LLM to generate, refine, or evaluate other prompts, enabling more dynamic, scalable, and autonomous workflows.
**Metadata**: Additional information stored alongside each prompt that provides context, usage guidelines, and performance insights.
**Metadata and Tagging**: Attaching descriptive information to runs and prompts to facilitate search, filtering, and analysis.
**Metric of Success**: A quantifiable measure that indicates whether a prompt variant is performing better.
**MLflow**: An open-source platform for managing the end-to-end machine learning lifecycle, including experiment tracking, reproducible runs, and model management.
**MLM (Masked Language Modeling)**: (for BERT-like models) predicting masked words.
**MLOps (Machine Learning Operations)**: Connecting prompt experimentation and management into the broader Machine Learning Operations (MLOps) lifecycle, which encompasses data management, model development, deployment, and monitoring.
**MLOps Integration**: Connecting prompt experimentation and management into the broader Machine Learning Operations (MLOps) lifecycle, which encompasses data management, model development, deployment, and monitoring.
**MLOps Platforms**: Tools like MLflow, Kubeflow, or DVC can help track prompt versions alongside model versions and data.
**Model Optimization**: Techniques like using smaller models, quantization, or distillation to reduce model size and inference cost.
**Monitoring**: The continuous collection and aggregation of metrics over time to track the health, performance, and usage patterns of your LLM application.
**Multi-Head Attention**: Instead of a single attention mechanism, Transformers use multiple "attention heads" in parallel.
**Negative Constraints**: Telling the Model What *Not* to Do
**New Relic**: An Application Performance Monitoring (APM) tool that combines metrics, logs, and traces for comprehensive application observability.
**NLP (Natural Language Processing)**: has undergone a significant transformation, evolving from rudimentary rule-based systems to sophisticated large language models (LLMs).
**Null Hypothesis (H0)**: States that there is no significant difference between the control and treatment groups. Any observed difference is due to random chance.
**Observability**: The ability to understand the internal state of your system from its external outputs, including comprehensive logging, real-time monitoring, and effective debugging tools.
**One-Shot Learning**: The prompt includes a single example of an input-output pair that demonstrates the desired task or format, followed by the actual query.
**OpenTelemetry**: A set of APIs, SDKs, and tools to instrument, generate, collect, and export telemetry data (metrics, logs, traces).
**Overlap Chunking**: Adding a small overlap (e.g., 10-20% of tokens) between consecutive chunks to preserve context across boundaries.
**p-value**: A common measure of statistical significance.
**PEFT (Parameter-Efficient Fine-Tuning)**: Techniques that allow for fine-tuning large models with fewer trainable parameters, reducing computational cost.
**Perplexity (PPL)**: a measure of how well a probability distribution or language model predicts a sample. In simpler terms, it quantifies how "surprised" the model is by a given sequence of words.
**phrasing**: you choose
**PII (Personally Identifiable Information)**: Information that can be used to identify, contact, or locate a single person, or can be directly linked to a person.
**Planner (Semantic Kernel)**: An LLM-powered component in Semantic Kernel that orchestrates calls to skills to achieve a user's goal.
**Pretraining**: To learn general language understanding, generation, and world knowledge from vast amounts of unlabeled text data.
**Prometheus**: An open-source monitoring system that collects metrics from configured targets.
**Prompt engineering**: the discipline of designing, refining, and optimizing the inputs (prompts) provided to large language models (LLMs) to achieve desired, accurate, and contextually relevant outputs.
**Prompt Injection**: A type of attack where malicious input is crafted to manipulate an LLM's behavior or extract sensitive information.
**Prompt Linting**: Automated static analysis of prompt text and structure to enforce coding standards, best practices, and identify potential issues early.
**Prompt Management Platforms**: Platforms for sharing, discovering, and managing LangChain prompts.
**Prompt Quality**: Akin to code quality.
**Prompt Registry**: A dedicated platform or database used to version, track, and deploy prompts.
**Prompt Registry / Library**: A centralized, versioned collection of all prompts, prompt templates, few-shot examples, and associated metadata used across an organization or project. It serves as the single source of truth for all prompt assets.
**Prompt Review Checklist**: A standardized set of criteria that reviewers use to evaluate prompt changes during a pull request.
**Prompt Templates (LangChain)**: Components in LangChain used to manage dynamic prompt construction.
**Prompt Versioning**: Treating prompts as code, storing them in version control, and using a dedicated prompt registry to track and deploy prompts.
**Prompting SDKs (Software Development Kits)**: Provide the necessary tools and abstractions to simplify building scalable, maintainable, and intelligent LLM-powered systems.
**Pull Requests (or Merge Requests) for Prompts**: Any change to a prompt (new prompt, update, deprecation) is proposed via a pull request.
**Quantization**: A technique for self-hosted models that reduces model size and inference cost by reducing the precision of model parameters.
**Query Engines (LlamaIndex)**: Components in LlamaIndex that combine retrieval and LLM generation for Q&A over data.
**RAG (Retrieval Augmented Generation)**: Embeddings are central to RAG systems.
**RAG (Retrieval-Augmented Generation)**: A technique that combines information retrieval with text generation to improve the factual accuracy and relevance of LLM outputs.
**Rate Limit Management**: Strategies to handle limits on the number of requests or tokens per minute/second imposed by LLM APIs, often involving retry logic with exponential backoff.
**Recursive Chunking**: A more advanced method that attempts to split text by larger units (e.g., sections, paragraphs) first, then recursively splits smaller units if they still exceed the limit.
**Relevance**: Does the output directly address the user's query or the prompt's intent? Is it on-topic?
**Repository Management**: Practices is essential for ensuring consistency, discoverability, reusability, and version control of your prompts at scale.
**Review Processes**: And a focus on **prompt quality** akin to code quality.
**RLHF (Reinforcement Learning from Human Feedback)**: Human evaluators rank model outputs, and this feedback is used to further optimize the model's behavior through reinforcement learning, aligning it with human values and instructions.
**Role Assignment**: Defining a persona for the LLM (e.g., "You are an expert historian") can significantly influence the style and content of its output.
**Roles and Responsibilities**: Clearly defined roles for prompt creation, review, and approval within the team.
**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: a recall-focused metric commonly used for evaluating summarization and machine translation. It measures the overlap of n-grams, word sequences, or word pairs between the generated summary and one or more reference summaries.
**Rule-Based and Symbolic Systems**: NLP systems relied on handcrafted rules, lexicons, and grammatical structures.
**Safety/Harmlessness**: Is the content free from bias, toxicity, hate speech, or other harmful outputs? (Critical for all applications).
**Scaled Dot-Product Attention**: A specific, efficient way to compute these attention scores.
**Self-Attention**: The core innovation of the Transformer. Each token in the input sequence computes an "attention score" with every other token.
**Self-Consistency**: Generate multiple CoT paths and then aggregate the final answers (e.g., by majority vote) to improve robustness.
**Self-Hosted Models**: Deploying open-source LLMs on your own infrastructure, either on dedicated servers (on-premises) or on cloud Virtual Machines (VMs) with GPUs.
**Semantic Similarity**: The cosine similarity between two embedding vectors indicates how semantically similar the corresponding texts are.
**Sentence/Paragraph Chunking**: Splitting text based on natural linguistic boundaries (sentences, paragraphs).
**SentencePiece**: Used by T5, handles multiple languages and allows for training a tokenizer directly from raw text.
**Sentry**: A real-time error monitoring tool that provides detailed stack traces and context for exceptions.
**Shadow Mode**: A deployment strategy where a new version of a service runs alongside the old version, processing live traffic but not affecting the live responses, used for testing.
**Skills/Plugins (Semantic Kernel)**: Collections of functions (native code or prompts) that the LLM can call within Semantic Kernel.
**Splunk**: A software platform used to search, analyze, and visualize machine-generated data.
**static templates and dynamic variables**: provides a fixed prompt structure with placeholders that are populated at runtime.
**Static Text**: The unchanging core instructions, context, or formatting guidelines that define the general task.
**Statistical NLP**: Shifted to machine learning models trained on large text corpora.
**Statistical Significance**: The probability that the observed difference between the control and treatment groups is not due to random chance, but rather a true effect of the change.
**Structured Logging**: Logging data in a consistent, machine-readable format (e.g., JSON) for easier parsing and analysis by log management systems.
**Summarization**: Condensing previous interactions or long documents into a shorter, more concise summary that can be fed back into the context window.
**T5 / Seq2Seq (Encoder-Decoder Models)**: These models utilize both a Transformer encoder and a decoder. They frame all NLP tasks as a "text-to-text" problem, where both input and output are text sequences.
**Tagging and Categorization**: Organizing prompts using a structured hierarchy and descriptive tags to improve searchability and management.
**Temperature**: a parameter that controls the randomness or "creativity" of the model's output.
**Testing Suite**: Unit tests for prompt rendering, end-to-end tests against a sandbox LLM.
**Text Splitters (LangChain)**: Utilities in LangChain to chunk documents for context management.
**Tokenization**: the process of converting raw text into smaller, discrete units called *tokens*.
**tokens**: the fundamental building blocks that LLMs operate on.
**Tone/Style Adherence**: Does the output match the requested tone (e.g., formal, friendly, witty) or style (e.g., journalistic, poetic)?
**Top-k sampling**: restricts the model's choice for the next token to only the `k` most probable tokens in the vocabulary.
**Top-p sampling (also known as nucleus sampling)**: selects the smallest set of tokens whose cumulative probability exceeds a threshold `p`.
**Tracing**: The process of following the execution path of a single request across multiple services and LLM calls to understand its flow and identify bottlenecks.
**Transformer**: architecture, introduced in 2017, revolutionized NLP by replacing recurrent and convolutional layers with attention mechanisms.
**Transformer Architecture**: (e.g., BERT, GPT) revolutionized NLP. Its self-attention mechanism allowed for parallel processing of sequences and superior capture of long-range dependencies.
**Treating Prompts as Code (Version Control)**: Manage prompt templates and configurations in a version control system (e.g., Git) alongside your application code.
**Treatment Group(s)**: The new variant(s) of the prompt or system that you are testing. You can have multiple treatment groups (A/B/n testing).
**Tree of Thought (ToT)**: A more advanced technique where the model explores multiple reasoning paths in a tree-like structure, allowing for backtracking and more systematic exploration of solutions.
**UAT (User Acceptance Testing)**: A phase of software testing in which the software is tested by the intended audience or users to ensure it meets their needs.
**Usefulness/Helpfulness**: Would a human user find this output genuinely helpful or actionable in a real-world scenario?
**vector database (or vector store)**: Your external knowledge base (documents, databases) is processed and converted into numerical embeddings (vector representations) and stored in a **vector database** (or vector store).
**Vector space**: The collection of all possible embeddings forms a high-dimensional *vector space*.
**Versioned Releases**: Tag stable prompt sets (e.g., `v1.0`) and maintain changelogs.
**Versioning**: Tracking changes to prompts over time, allowing for rollbacks to previous versions, A/B testing of new versions, and clear understanding of what prompt is deployed where.
**Visualization and Dashboards**: Using interactive dashboards to compare experiment runs, visualize performance trends, and identify optimal prompt configurations.
**Voting**: a simpler ensemble technique primarily used for classification, categorization, or selection tasks.
**VS Code Notebooks**: Jupyter notebook integration within Visual Studio Code.
**Weights & Biases (W&B)**: A popular MLOps platform for experiment tracking, visualization, and collaboration.
**Weights & Biases Prompts**: An MLOps platform specifically designed for logging, tracing, and monitoring prompt inputs, outputs, and metrics.
**Widgets (ipywidgets)**: Interactive elements in Jupyter Notebooks used to create interactive controls like sliders or dropdowns for prompt parameters.
**WordPiece**: Used by BERT, similar to BPE but based on likelihood.
**ZenML / Kedro (Orchestration Frameworks)**: MLOps frameworks that help build and orchestrate end-to-end machine learning pipelines.
**Zero-Shot CoT**: where you merely append a phrase like "Let's think step by step" or "Think step by step and then provide the answer" to your original prompt.
**Zero-Shot Learning**: The model performs a task solely based on the natural language instruction provided in the prompt, without any explicit examples of input-output pairs for that specific task.
