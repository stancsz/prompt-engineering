# Appendix B: Glossary of Terms & Acronyms

A quick reference for key terminology used throughout this book.

- **LLM** (Large Language Model): A deep learning model trained on vast text corpora to generate or understand language.  
- **Prompt**: The input text or instruction given to an LLM to guide its output.  
- **Zero-Shot**: Asking the model to perform a task without any examples.  
- **Few-Shot**: Providing a small number of examples to illustrate the task.  
- **Chain-of-Thought**: A prompting technique where the model is asked to show its reasoning steps.  
- **RAG** (Retrieval-Augmented Generation): Combining information retrieval with LLM generation by prepending retrieved context.  
- **Token**: A unit of text (words or subwords) used by LLMs for processing.  
- **Temperature**: A decoding parameter controlling randomness in generation.  
- **Top-k / Top-p**: Sampling techniques limiting candidate tokens by rank (k) or cumulative probability (p).  
- **Soft Prompt**: A continuous embedding vector learned to steer model behavior.  
- **Adapter**: A small trainable module inserted into transformer layers for efficient fine-tuning.  
- **Prefix Tuning**: Learning a continuous “prefix” of key-value pairs to guide model outputs without full fine-tuning.  
- **Hallucination**: When an LLM generates plausible-sounding but incorrect or fabricated content.  
- **Prompt Injection**: A security vulnerability where user input alters system-level instructions.  
- **Sandboxing**: Isolating user input from system instructions to mitigate injection risks.  
- **CI/CD**: Continuous Integration and Continuous Deployment—automating testing and release workflows.  
- **MLOps**: Practices and tooling for operationalizing machine learning workflows in production.  
- **A/B Test**: A controlled experiment comparing two prompt variants to determine which performs better.  
- **BLEU/ROUGE**: Automated metrics for evaluating generated text against reference outputs.  
- **Perplexity**: A measure of how well a probability model predicts a sample—lower is better.  
- **Embedding**: A dense vector representation of text capturing semantic meaning.  
- **Beam Search**: A decoding strategy that keeps the top N candidate sequences at each step.
