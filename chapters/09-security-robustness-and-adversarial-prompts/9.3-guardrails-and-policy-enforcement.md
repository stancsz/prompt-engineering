# 9.3 Guardrails & Policy Enforcement

Guardrails define permissible behaviors and filter unsafe or disallowed outputs.

## Key Concepts

- **Policy Rules:** High‐level guidelines (e.g., no hate speech, no medical advice).  
- **Hard vs. Soft Filters:**  
  - Hard: Block or truncate disallowed content.  
  - Soft: Warn or flag outputs for human review.  
- **Role‐Based Enforcement:** Separate “system” layer enforces policies before generation.  
- **Automated Post‐Processing:** Use regex, keyword lists, or classifier models to scan responses.

## Example Guardrail Patterns

1. **Blacklist Words:**  
   ```
   if any(word in output for word in ["bomb", "kill"]):  
       raise SafetyError
   ```
2. **Classifier Check:**  
   ```python
   from safety_model import ToxicityClassifier
   if ToxicityClassifier.predict(output) > 0.7:
       return "[Content removed: unsafe]"
   ```
3. **Policy Prompting:** Embed policy in system prompt:  
   ```
   SYSTEM: “You must refuse to generate content that is violent or hateful.”
   ```

## Hands-On Exercise

1. Define a short policy (e.g., no medical advice).  
2. Create a malicious user prompt asking for medical treatment.  
3. Implement:
   - A system‐level refusal template  
   - A post‐generation sanitizer  
   - A classifier‐based filter  
4. Compare how each approach handles the request.

## Reflection

- Which guardrail was most reliable at stopping disallowed content?  
- How did user experience differ when using hard vs. soft filters?  
- What trade‐offs exist between strict enforcement and user freedom?
