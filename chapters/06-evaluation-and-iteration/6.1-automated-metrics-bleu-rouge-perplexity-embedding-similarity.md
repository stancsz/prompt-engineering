# 6.1 Automated Metrics: BLEU, ROUGE, Perplexity, and Embedding Similarity

Effective prompt engineering is an iterative process that requires objective evaluation. While human judgment is invaluable (Chapter 6.2), **automated metrics** provide quantitative ways to measure the performance of LLM outputs, compare different prompt versions, and track improvements over time. These metrics are particularly useful for large-scale testing and integration into CI/CD pipelines.

## 1. BLEU (Bilingual Evaluation Understudy)

*   **Definition:** BLEU is a precision-focused metric primarily used for evaluating machine translation. It measures the n-gram overlap between the generated text (hypothesis) and one or more human-written reference translations.
*   **Mechanism:** It counts how many n-grams (sequences of N words) in the generated text appear in the reference text. It also includes a brevity penalty to penalize overly short translations.
*   **Range:** 0 to 1 (or 0 to 100). Higher scores indicate better quality.
*   **Use Cases:** Machine translation, text summarization (less common than ROUGE), any task where exact phrase matching is important.
*   **Limitations:**
    *   **Exact Match Bias:** Heavily relies on exact word/n-gram matches, penalizing grammatically correct but paraphrased outputs.
    *   **Insensitive to Meaning:** Can give a high score to a grammatically incorrect sentence if it shares many words with the reference.
    *   **Requires Multiple References:** More reliable with multiple human references to capture linguistic variability.

## 2. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

*   **Definition:** ROUGE is a recall-focused metric commonly used for evaluating summarization and machine translation. It measures the overlap of n-grams, word sequences, or word pairs between the generated summary and one or more reference summaries.
*   **Mechanism:**
    *   **ROUGE-N:** Measures overlap of N-grams (e.g., ROUGE-1 for unigrams, ROUGE-2 for bigrams).
    *   **ROUGE-L:** Measures the longest common subsequence (LCS), capturing sentence-level structure.
    *   **ROUGE-S:** Measures skip-bigram co-occurrence.
*   **Range:** 0 to 1 (or 0 to 100). Higher scores indicate better quality.
*   **Use Cases:** Text summarization (most common), question answering, paraphrase detection.
*   **Limitations:** Similar to BLEU, it struggles with semantic understanding and requires reference summaries.

**Example: Calculating BLEU and ROUGE in Python**
*Requires `sacrebleu` and `rouge-score` libraries (`pip install sacrebleu rouge-score`)*

```python
from sacrebleu import corpus_bleu
from rouge_score import rouge_scorer

# Example: Summarization Task
reference_summary = "The quick brown fox jumps over the lazy dog."
generated_summary = "A fast brown fox jumps over a sleepy dog."
another_generated_summary = "The fox jumps over the dog."

# BLEU (requires list of references, even if only one)
references_for_bleu = [[reference_summary]]
hypotheses_for_bleu = [generated_summary]
bleu_score = corpus_bleu(hypotheses_for_bleu, references_for_bleu)
print(f"BLEU Score: {bleu_score.score:.2f}") # Output: BLEU Score: 40.82 (example value)

# ROUGE (scorer expects single strings for hypothesis and reference)
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

# Score for generated_summary
scores_gen1 = scorer.score(reference_summary, generated_summary)
print(f"ROUGE-1 (Gen1): {scores_gen1['rouge1'].fmeasure:.2f}")
print(f"ROUGE-L (Gen1): {scores_gen1['rougeL'].fmeasure:.2f}")

# Score for another_generated_summary
scores_gen2 = scorer.score(reference_summary, another_generated_summary)
print(f"ROUGE-1 (Gen2): {scores_gen2['rouge1'].fmeasure:.2f}")
print(f"ROUGE-L (Gen2): {scores_gen2['rougeL'].fmeasure:.2f}")
```

## 3. Perplexity (PPL)

*   **Definition:** Perplexity is a measure of how well a probability distribution or language model predicts a sample. In simpler terms, it quantifies how "surprised" the model is by a given sequence of words. A lower perplexity score indicates that the model is better at predicting the text, meaning the text is more probable according to the model.
*   **Mechanism:** It's the exponential of the cross-entropy loss.
*   **Use Cases:** Primarily used to evaluate the quality of a language model itself, or to compare how different prompts influence the *likelihood* of the generated text *given the model*. It's less about task-specific quality and more about fluency and naturalness according to the model's internal probabilities.
*   **Limitations:**
    *   Requires access to the model's log-probabilities for generated tokens.
    *   Does not directly measure factual correctness, relevance, or adherence to specific instructions.
    *   Can be misleading if comparing models trained on different data.

**Hands-On Exercise: Conceptual Perplexity Calculation**
*Note: Direct perplexity calculation for arbitrary text is often not exposed via public LLM APIs. This is more for understanding the concept.*

1.  **Conceptual API Call:** Imagine an API that returns `logprobs` for each token.
    ```bash
    # This is a conceptual curl command, actual API might vary
    curl https://api.openai.com/v1/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -d '{
        "model": "gpt-3.5-turbo-instruct", # or a model that supports logprobs
        "prompt": "The quick brown fox jumps over the lazy dog.",
        "max_tokens": 0, # Only get logprobs for the input
        "echo": true,
        "logprobs": 1 # Request log probabilities
      }'
    ```
2.  **Compute Perplexity (Python Snippet):**
    ```python
    import math

    # Assume you get log_probs from an API response for a sequence of tokens
    # Example log_probs (hypothetical values for "The quick brown fox")
    log_probs = [-0.05, -0.1, -0.08, -0.03] # These are log probabilities for each token

    # Sum of log probabilities
    sum_log_probs = sum(log_probs)

    # Number of tokens
    num_tokens = len(log_probs)

    # Average negative log likelihood
    avg_nll = -sum_log_probs / num_tokens

    # Perplexity
    perplexity = math.exp(avg_nll)

    print(f"Perplexity: {perplexity:.2f}")
    ```
    *Interpretation:* A lower perplexity means the model found the sequence more predictable/natural.

## 4. Embedding Similarity

*   **Definition:** Measures the semantic similarity between the embedding vector of the generated text and the embedding vector of a reference text (or query).
*   **Mechanism:** Both the generated text and the reference text are converted into dense numerical embeddings (Chapter 2.3). The cosine similarity between these two vectors is then calculated. A score closer to 1 indicates higher semantic similarity.
*   **Use Cases:**
    *   **Semantic Relevance:** Evaluating if a generated answer is semantically relevant to a question, even if it doesn't use exact keywords.
    *   **Summarization:** Assessing if a summary captures the core meaning of the original text.
    *   **Paraphrase Detection:** Determining if two sentences convey the same meaning.
    *   **RAG Evaluation:** Checking if retrieved chunks are semantically relevant to the query.
*   **Benefits:** Overcomes the limitations of exact word overlap metrics (BLEU/ROUGE) by focusing on meaning.
*   **Limitations:** Does not directly measure factual correctness or adherence to specific instructions. Can be fooled by semantically similar but factually incorrect statements.

**Hands-On Exercise: Calculating Embedding Similarity**
*Requires `openai` and `numpy` libraries (`pip install openai numpy`)*

```python
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os

client = OpenAI() # Ensure OPENAI_API_KEY is set

def get_embedding(text, model="text-embedding-ada-002"):
    text = text.replace("\n", " ") # Replace newlines for better embedding
    return client.embeddings.create(input=[text], model=model).data[0].embedding

reference_text = "The quick brown fox jumps over the lazy dog."
generated_text_1 = "A swift fox leaps over a tired canine." # Semantically similar
generated_text_2 = "The cat sat on the mat." # Semantically dissimilar

embedding_ref = get_embedding(reference_text)
embedding_gen1 = get_embedding(generated_text_1)
embedding_gen2 = get_embedding(generated_text_2)

# Reshape for cosine_similarity function
embedding_ref_np = np.array(embedding_ref).reshape(1, -1)
embedding_gen1_np = np.array(embedding_gen1).reshape(1, -1)
embedding_gen2_np = np.array(embedding_gen2).reshape(1, -1)

similarity_1 = cosine_similarity(embedding_ref_np, embedding_gen1_np)[0][0]
similarity_2 = cosine_similarity(embedding_ref_np, embedding_gen2_np)[0][0]

print(f"Similarity (Reference vs. Gen1): {similarity_1:.4f}")
print(f"Similarity (Reference vs. Gen2): {similarity_2:.4f}")
```
*Interpretation:* `similarity_1` should be high (e.g., >0.8), while `similarity_2` should be low (e.g., <0.5).

## Limitations of Automated Metrics (General)

*   **Lack of Human Judgment:** No automated metric can fully capture the nuances of human perception of quality, creativity, or helpfulness.
*   **Reference Dependency:** Most metrics require one or more "ground truth" reference answers, which can be expensive and time-consuming to create, especially for generative tasks.
*   **Creativity vs. Accuracy:** Metrics often struggle to evaluate creative outputs, as there's no single "correct" answer.
*   **Context Sensitivity:** They may not fully account for how well the model uses or ignores specific contextual cues in the prompt.

## When to Use Which Metric

*   **Machine Translation:** BLEU (primary), ROUGE.
*   **Summarization:** ROUGE (primary), Embedding Similarity.
*   **Factual Question Answering:** Exact match (for short answers), Embedding Similarity (for longer answers), Perplexity (for fluency).
*   **Text Generation (Creative):** Less reliance on automated metrics; human evaluation is often preferred. Embedding similarity can check for thematic consistency.
*   **Classification:** Accuracy, Precision, Recall, F1-score (standard classification metrics).

## Hands-On Exercise: Evaluating a Summarization Prompt

1.  **Choose an Article and Write a Reference Summary:** Select a short news article (e.g., 200-300 words). Write a concise, 3-sentence human-quality summary of it. This is your `reference_summary`.
2.  **Generate LLM Summaries:**
    *   Craft a prompt to summarize the article (e.g., "Summarize the following article in three sentences: [article text]").
    *   Generate 3-5 summaries using your LLM (e.g., with `temperature=0.7` to get some variation). These are your `generated_summaries`.
3.  **Calculate Metrics:**
    *   For each `generated_summary`, calculate its ROUGE-L F1-score against your `reference_summary`.
    *   For each `generated_summary`, calculate its Embedding Similarity against your `reference_summary`.
4.  **Analyze and Reflect:**
    *   Which generated summary scored highest on ROUGE-L? Which on Embedding Similarity?
    *   Do the scores align with your human judgment of which summary is best? Where do they diverge?
    *   How might you use these metrics to iteratively improve your summarization prompt?

## Best Practices for Automated Evaluation

*   **Define Clear Objectives:** Before choosing metrics, clearly define what "good" output means for your specific task.
*   **Use Multiple Metrics:** Relying on a single metric can be misleading. Use a combination (e.g., ROUGE + Embedding Similarity for summarization).
*   **Establish Baselines:** Compare your prompt's performance against a simple baseline (e.g., a naive prompt, or a non-LLM method).
*   **Human-in-the-Loop:** Always complement automated metrics with human evaluation, especially for subjective tasks.
*   **Track Over Time:** Monitor metrics over time to detect regressions or measure the impact of prompt changes.
*   **Automate the Pipeline:** Integrate metric calculation into your prompt engineering workflow (e.g., using Python scripts or prompt management platforms).
*   **Consider Context:** Remember that metrics are tools, not the ultimate goal. Interpret scores within the context of your application's needs.
