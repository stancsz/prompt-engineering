# 6.1 Automated Metrics: BLEU, ROUGE, Perplexity, Embedding Similarity

Quantitative metrics help you measure prompt performance and compare revisions objectively.

## BLEU & ROUGE

- **BLEU**: Precision-focused; counts n-gram overlap between generated text and reference.  
- **ROUGE**: Recall-focused; measures how much reference text is covered by output.

### Example

```bash
# Compute BLEU/ROUGE with sacrebleu and rouge-score in Python
from sacrebleu import corpus_bleu
from rouge_score import rouge_scorer

refs = [["The cat sat on the mat."]]
hyps = ["The cat is sitting on the mat."]
print(corpus_bleu(hyps, refs))
scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)
print(scorer.score(refs[0][0], hyps[0]))
```

## Perplexity

- **Definition**: Exponential of cross-entropy; lower = model finds text more probable.  
- **Use Case**: Compare how “surprising” different prompts make the model’s outputs.

### Hands-On

1. Generate log-probabilities via API:  
   ```bash
   curl https://api.openai.com/v1/completions \
     -d '{"model":"gpt-4","prompt":"Hello","echo":true,"logprobs":0}'
   ```  
2. Compute perplexity from returned token log-probs.

## Embedding Similarity

- **Definition**: Cosine similarity between embeddings of reference and generated text.  
- **Use Case**: Semantic alignment when exact overlap metrics fail.

### Hands-On

1. Embed both texts via OpenAI Embedding API.  
2. Compute cosine similarity in a notebook.

## Reflection

- Which metric best captured quality for your task?  
- How did metrics guide iterative prompt changes?  
- When did automated scores diverge from human judgments?
