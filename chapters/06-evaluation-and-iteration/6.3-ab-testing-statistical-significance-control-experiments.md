# 6.3 A/B Testing, Statistical Significance & Control Experiments

A/B testing lets you compare two or more prompt variants under controlled conditions to identify the best performer.

## Concepts

- **A/B Test:** Split traffic or runs evenly between prompt A and prompt B.  
- **Control Group:** The baseline prompt you compare others against.  
- **Treatment Group:** The variant(s) you are testing.  
- **Statistical Significance:** The probability that observed differences are not due to chance (p‐value < 0.05).

## Example Workflow

1. **Define Metrics:** Choose success criteria (e.g., accuracy, BLEU score, user rating).  
2. **Select Variants:**  
   - A: “Summarize in 3 bullets.”  
   - B: “Summarize in 5 bullets.”  
3. **Run Experiment:**  
   - Generate N outputs per variant (e.g., 100 each).  
4. **Analyze Results:**  
   - Compute mean metric per group.  
   - Use a t-test or chi-square to calculate p-value.

## Hands-On Exercise

1. Choose a classification or summarization prompt.  
2. Create two variants that differ in one detail (length, role, examples).  
3. Generate 50 completions per variant.  
4. Measure a simple metric (e.g., number of bullet points, labeling accuracy).  
5. Compute statistical significance in Python:
   ```python
   from scipy.stats import ttest_ind
   t_stat, p_val = ttest_ind(scores_A, scores_B)
   print(f"p-value: {p_val}")
   ```
6. Conclude which variant is significantly better.

## Reflection

- Was the difference between A and B meaningful or due to noise?  
- How many samples were needed to reach significance?  
- What changes will you promote to your production prompt?
