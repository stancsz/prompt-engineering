# 6.2 Human Evaluation: Criteria, Surveys & Feedback Loops

Human judgments remain the gold standard for many prompt evaluation tasks.

## Evaluation Criteria

- **Fluency:** Is the generated text natural and grammatical?  
- **Relevance:** Does the output address the user’s intent?  
- **Accuracy:** Are facts and information correct?  
- **Coherence:** Does the response flow logically?  
- **Usefulness:** Would a human find this output helpful?

## Designing Surveys

1. **Define Goals:** Identify which criteria matter for your application.  
2. **Scale Selection:** Use Likert scales (e.g., 1–5) or binary judgments.  
3. **Questionnaire:**  
   - “Rate how relevant this response is on a scale of 1 (not at all) to 5 (very).”  
   - “Would you use this text in a report? Yes/No.”

## Feedback Loop

1. **Collect Samples:** Generate candidate outputs for evaluation.  
2. **Gather Ratings:** Use mechanical turk, internal team, or partner feedback.  
3. **Analyze Results:** Identify prompt patterns correlated with high scores.  
4. **Iterate Prompts:** Refine phrasing, examples, or constraints based on feedback.

## Hands-On Exercise

1. Generate 10 outputs for the same prompt.  
2. Create a simple survey form (Google Forms or similar) with rating questions.  
3. Invite 3–5 colleagues/friends to rate each response.  
4. Summarize average scores and note common critiques.

## Reflection

- Which criteria showed the most variance?  
- How did human ratings compare to automated metrics?  
- What prompt adjustments emerged from qualitative feedback?
