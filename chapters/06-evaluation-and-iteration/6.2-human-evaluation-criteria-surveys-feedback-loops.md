# 6.2 Human Evaluation: Criteria, Surveys, and Feedback Loops

While automated metrics (Chapter 6.1) provide quantitative insights, they often fall short in capturing the subjective nuances of human language, creativity, and real-world usefulness. For many prompt engineering tasks, especially those involving open-ended generation, complex reasoning, or user-facing applications, **human evaluation** remains the gold standard. This chapter explores how to define evaluation criteria, design effective surveys, and establish continuous feedback loops to iteratively improve LLM performance.

## Why Human Evaluation is Indispensable

*   **Subjective Quality:** Captures aspects like tone, style, creativity, and engagement that automated metrics cannot.
*   **Nuance and Context:** Humans can understand subtle contextual cues and implicit meanings that LLMs might miss.
*   **Real-World Usefulness:** Directly assesses whether the LLM's output is helpful, actionable, or appropriate for its intended purpose.
*   **Error Detection:** Identifies subtle errors, biases, or hallucinations that automated checks might overlook.
*   **Alignment with User Needs:** Ensures the LLM's behavior aligns with actual user expectations and preferences.

## Key Human Evaluation Criteria

The specific criteria will vary based on your application, but common ones include:

1.  **Fluency/Readability:** Is the generated text grammatically correct, natural-sounding, and easy to read?
2.  **Relevance:** Does the output directly address the user's query or the prompt's intent? Is it on-topic?
3.  **Accuracy/Factuality:** Is the information presented factually correct and free from hallucinations? (Crucial for factual tasks).
4.  **Coherence/Consistency:** Does the response flow logically? Is it internally consistent? Does it maintain consistency with previous turns in a conversation?
5.  **Completeness:** Does the response provide all necessary information to fulfill the request?
6.  **Conciseness:** Is the response brief and to the point, without unnecessary verbosity? (Important for summarization, extraction).
7.  **Usefulness/Helpfulness:** Would a human user find this output genuinely helpful or actionable in a real-world scenario?
8.  **Safety/Harmlessness:** Is the content free from bias, toxicity, hate speech, or other harmful outputs? (Critical for all applications).
9.  **Tone/Style Adherence:** Does the output match the requested tone (e.g., formal, friendly, witty) or style (e.g., journalistic, poetic)?
10. **Creativity/Novelty:** For generative tasks, is the output original, imaginative, and not repetitive?

## Designing Effective Human Evaluation Surveys

### 1. Define Clear Goals and Criteria

*   Before designing, clearly articulate *what* you want to measure and *why*. Which criteria are most important for your application's success?
*   Translate abstract criteria into concrete, measurable questions.

### 2. Select Appropriate Evaluators

*   **Internal Experts:** Subject matter experts, product managers, or prompt engineers. Provide high-quality, nuanced feedback but can be expensive and slow.
*   **Crowd-sourcing Platforms:** (e.g., Amazon Mechanical Turk, Appen, Scale AI). Cost-effective for large volumes but requires careful task design and quality control.
*   **End-Users:** Direct feedback from actual users of your application (e.g., thumbs up/down buttons, feedback forms). Provides invaluable real-world insights.

### 3. Choose Rating Scales and Question Types

*   **Likert Scales (e.g., 1-5):** For subjective criteria like fluency, relevance, usefulness.
    *   *Example:* "On a scale of 1 (Very Irrelevant) to 5 (Very Relevant), how relevant is this response to the question?"
*   **Binary Judgments (Yes/No):** For objective criteria or quick filtering.
    *   *Example:* "Is this response factually accurate? Yes/No."
    *   *Example:* "Would you use this response as is? Yes/No."
*   **Multiple Choice:** For classification tasks.
*   **Open-Ended Text Fields:** Crucial for collecting qualitative feedback, explanations for low ratings, and suggestions for improvement.

### 4. Structure the Questionnaire

*   **Clear Instructions:** Provide concise, unambiguous instructions for evaluators. Define each criterion clearly.
*   **Randomization:** Randomize the order of responses to prevent bias.
*   **Blind Evaluation:** If possible, hide which prompt version generated which output to prevent bias.
*   **Side-by-Side Comparison:** For A/B testing (Chapter 6.3), present two outputs side-by-side and ask evaluators to choose which is better or rate both.
*   **Context Provision:** Provide the original prompt and any relevant context (e.g., previous conversation turns) to the evaluators.
*   **Avoid Leading Questions:** Phrase questions neutrally.

### 5. Tools for Survey Creation

*   **Google Forms, SurveyMonkey:** Simple for small-scale internal evaluations.
*   **Specialized Annotation Platforms:** Prodigy, Label Studio, or crowd-sourcing platforms for larger, more complex annotation tasks.

## The Human Feedback Loop: Continuous Improvement

Human evaluation is not a one-off activity but a continuous process integrated into the prompt engineering lifecycle.

1.  **Collect Samples:** Generate a diverse set of LLM outputs using different prompts, inputs, and decoding parameters.
2.  **Gather Ratings:** Distribute these samples to your chosen evaluators via surveys or annotation tasks.
3.  **Analyze Results:**
    *   **Quantitative Analysis:** Calculate average scores for each criterion, identify trends, and compare performance across different prompt versions.
    *   **Qualitative Analysis:** Read through open-ended comments. Look for recurring themes, specific examples of good/bad outputs, and actionable suggestions.
    *   **Error Categorization:** Classify specific types of errors (e.g., hallucination, off-topic, formatting error) to pinpoint areas for prompt improvement.
4.  **Iterate Prompts:** Based on the analysis, refine your prompts. This might involve:
    *   Adjusting instructions for clarity.
    *   Adding/modifying few-shot examples.
    *   Changing roles or constraints.
    *   Implementing new prompt patterns (e.g., CoT, RAG).
    *   Adjusting decoding parameters.
5.  **Re-evaluate:** Test the refined prompts with a new set of samples and repeat the evaluation process.

## Hands-On Exercise: Conducting a Mini Human Evaluation

1.  **Choose a Generative Task:** Select a task where automated metrics are insufficient, such as generating creative headlines for a product or writing short social media posts.
2.  **Generate Outputs:**
    *   Craft two different prompt versions for the same task (e.g., one simple, one with a role and constraints).
    *   Generate 5-10 outputs for each prompt version using the same input.
3.  **Design a Simple Survey:**
    *   Use Google Forms or a similar tool.
    *   For each generated output, include:
        *   The original prompt and input.
        *   Rating questions (e.g., 1-5 for "Creativity," "Relevance," "Readability").
        *   An open-ended text box for "Comments/Suggestions."
    *   Crucially, *do not* reveal which prompt version generated which output.
4.  **Gather Feedback:** Invite 3-5 friends or colleagues to complete the survey.
5.  **Analyze and Reflect:**
    *   Calculate average scores for each prompt version across all criteria.
    *   Read all qualitative comments. What common themes emerge?
    *   Which prompt version performed better according to human judgment? Why?
    *   Based on the feedback, what specific changes would you make to your prompts?

## Challenges of Human Evaluation

*   **Cost:** Can be expensive, especially for large-scale evaluations or when using expert annotators.
*   **Time-Consuming:** Collecting and analyzing human feedback takes time.
*   **Subjectivity:** Human judgments can be subjective and vary between annotators. Requires clear guidelines and potentially inter-annotator agreement checks.
*   **Scalability:** Difficult to scale to the same extent as automated metrics.
*   **Bias:** Evaluators can introduce their own biases.

## Best Practices for Human Evaluation

*   **Clear Guidelines:** Provide detailed, unambiguous instructions and definitions for evaluators.
*   **Training:** Train annotators on the evaluation criteria and task.
*   **Inter-Annotator Agreement:** Measure agreement between multiple evaluators to ensure consistency.
*   **Randomization and Blinding:** Randomize presentation order and blind evaluators to prompt versions.
*   **Mix Quantitative and Qualitative:** Use rating scales for broad trends and open-ended comments for actionable insights.
*   **Iterate Frequently:** Integrate human feedback into your prompt development cycle as early and often as possible.
*   **Balance with Automated Metrics:** Use automated metrics for initial filtering and large-scale tracking, and human evaluation for nuanced quality assessment.
