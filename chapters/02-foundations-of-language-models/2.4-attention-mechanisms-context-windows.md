# 2.4 Attention Mechanisms & Context Windows

Attention lets models focus on relevant tokens; context windows limit how much text the model “sees.”

## Attention Mechanisms

- **Self-Attention**: Each token attends to every other token to compute representations.  
- **Scaled Dot-Product**: Computes attention weights via dot products, scaled by √d.  
- **Multi-Head**: Parallel attention “heads” capture different relationships.

### Example

```text
Input: “The cat sat on the mat because it was tired.”
Self-attention weight from “it” → “cat” highlights coreference.
```

## Context Windows

- **Definition**: Maximum number of tokens the model processes at once.  
- **Implications**: Longer prompts + examples may overflow and truncate.  
- **Workarounds**: Summarize earlier context, use retrieval-augmented prompts.

### Hands-On Exercise

1. In Playground, enter a prompt with 1,000 tokens of context and observe truncation.  
2. Replace early paragraphs with a summary—compare completion quality.  
3. Try a 4,000-token context window model vs. a 2,000-token one.

## Reflection

- How did attention weights help resolve ambiguities?  
- What strategies prevented important context from being cut off?
