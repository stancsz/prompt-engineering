# 2.1 Model Architectures (Transformer, GPT, BERT, etc.)

Modern NLP relies on neural architectures that process sequences of tokens and capture context.

## Key Architectures

- **Transformer**: Attention-based encoder–decoder.  
- **GPT (Generative Pretrained Transformer)**: Decoder-only, autoregressive.  
- **BERT (Bidirectional Encoder Representations from Transformers)**: Encoder-only, bidirectional.  
- **T5 / Seq2Seq**: Unified text‐to‐text using encoder–decoder.

## How They Differ

| Feature           | Transformer   | GPT           | BERT          |
|-------------------|---------------|---------------|---------------|
| Directionality    | Both          | Left-to-right | Bidirectional |
| Use Case          | Translation   | Generation    | Classification|
| Pretraining Task  | Masked tokens | Next token    | Masked LM     |

## Example Prompt Differences

- GPT:  
  ```
  You are a storyteller. Continue this sentence: "Once upon a time, a curious AI…"  
  ```  
- BERT (mask filling):  
  ```
  Fill in the blank: "The capital of France is [MASK]."  
  ```

## Hands-On Exercise

1. In an LLM playground, compare a GPT-style completion vs. a BERT mask fill.  
2. Try masking different parts of a sentence (“Today is a [MASK] day”).  
3. Observe how context length and model type affect accuracy.

## Reflection

- Which architecture gave more coherent continuations?  
- How did bidirectional context improve mask filling?
