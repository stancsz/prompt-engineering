# 2.3 Tokenization, Embeddings, and Vector Spaces

Understanding how text is broken into tokens and mapped into vectors is critical for prompt engineering.

## Tokenization

- **Definition:** Splitting raw text into discrete units (tokens).  
- **Methods:** Byte-Pair Encoding (BPE), WordPiece, SentencePiece.  
- **Impact on Prompts:** Length limits enforced in tokens, not characters.  
- **Hands-On:**  
  1. In OpenAI Playground, paste “Prompt engineering” and check token count.  
  2. Compare with “Prompt engineering is fun!” — note extra tokens for punctuation and spaces.

## Embeddings

- **Definition:** Dense vector representations of tokens or entire sentences.  
- **Use Cases:** Semantic search, similarity, clustering.  
- **Tools:** OpenAI Embedding API, Sentence-BERT.  
- **Hands-On:**  
  1. Generate embeddings for two sentences: “AI is powerful.” vs. “Artificial intelligence is awesome.”  
  2. Compute cosine similarity (use a notebook snippet).  

```python
from sklearn.metrics.pairwise import cosine_similarity
# example vectors a, b
similarity = cosine_similarity([a], [b])
print(similarity)
```

## Vector Spaces

- **Concept:** Each token or sentence is a point in a high-dimensional space.  
- **Visualization:** Use PCA or t-SNE to project embeddings to 2D.  
- **Hands-On:**  
  1. Fetch 50 embedding vectors for product reviews.  
  2. Plot with t-SNE in a Jupyter notebook to see clusters by sentiment.

## Reflection

- How does tokenization affect prompt length and truncation?  
- What patterns emerge when visualizing embeddings?
