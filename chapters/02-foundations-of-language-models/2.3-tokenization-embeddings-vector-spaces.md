# 2.3 Tokenization, Embeddings, and Vector Spaces

To effectively engineer prompts, it is crucial to understand how Large Language Models (LLMs) process and represent text internally. This involves three core concepts: tokenization, embeddings, and vector spaces. These mechanisms transform human-readable text into a numerical format that LLMs can understand and manipulate, directly impacting prompt length, context understanding, and retrieval capabilities.

## 1. Tokenization: Breaking Down Text

**Definition:** Tokenization is the process of converting raw text into smaller, discrete units called *tokens*. These tokens are the fundamental building blocks that LLMs operate on. A token can be a word, a subword, a character, or even a punctuation mark.

*   **Why it's necessary:** LLMs are mathematical models that operate on numbers, not raw text. Tokenization bridges this gap by mapping text to numerical IDs.
*   **Common Methods:**
    *   **Word-based:** Simple splitting by spaces, but struggles with morphology (e.g., "running," "ran").
    *   **Character-based:** Treats each character as a token, leading to very long sequences.
    *   **Subword Tokenization (most common for LLMs):**
        *   **Byte-Pair Encoding (BPE):** Iteratively merges the most frequent pairs of characters or subwords.
        *   **WordPiece:** Used by BERT, similar to BPE but based on likelihood.
        *   **SentencePiece:** Used by T5, handles multiple languages and allows for training a tokenizer directly from raw text.
    *   Subword tokenization balances vocabulary size and sequence length, handling rare words by breaking them into common subwords.
*   **Impact on Prompts:**
    *   **Context Window Limits:** LLMs have a maximum number of tokens they can process in a single input (the "context window"). Understanding how your prompt translates into tokens is vital to avoid truncation or exceeding limits.
    *   **Cost:** Many LLM APIs charge based on token usage (both input and output). Efficient tokenization can reduce operational costs.
    *   **Granularity:** The choice of tokenizer can affect how the model interprets nuances, especially with rare words or domain-specific terms.

**Hands-On Exercise: Observing Tokenization**
1.  **Access a Tokenizer Tool:** Use an online tokenizer (e.g., OpenAI's Tokenizer, Hugging Face's Tokenizer Playground) or a Python library (e.g., `transformers` library).
2.  **Simple Text:** Paste the phrase "Prompt engineering is fascinating!"
    *   Note the token count.
    *   Observe how punctuation and spaces are handled.
3.  **Complex Word:** Try a long, uncommon word like "antidisestablishmentarianism" or a technical term.
    *   See how it's broken down into subwords.
    *   Compare the token count to the character count.
4.  **Code Snippet:** Paste a short code snippet (e.g., `def factorial(n): return 1 if n == 0 else n * factorial(n-1)`).
    *   How does the tokenizer handle keywords, variable names, and symbols?

## 2. Embeddings: Semantic Representation

**Definition:** After tokenization, each token (or sequence of tokens) is converted into a dense numerical vector called an *embedding*. These embeddings are high-dimensional representations where the position and proximity of vectors in the space encode semantic meaning.

*   **Concept:** Words or phrases with similar meanings or contexts are mapped to vectors that are close to each other in the embedding space. For example, the embedding for "king" will be closer to "queen" than to "apple."
*   **Properties:**
    *   **Semantic Similarity:** The cosine similarity between two embedding vectors indicates how semantically similar the corresponding texts are.
    *   **Contextualization:** Modern LLMs generate *contextual embeddings*, meaning the embedding for a word like "bank" will differ depending on whether it refers to a financial institution or a river bank.
*   **Use Cases in Prompt Engineering:**
    *   **Retrieval Augmented Generation (RAG):** Embeddings are central to RAG systems (covered in Chapter 5). By embedding a user's query and a knowledge base, you can find semantically relevant documents to augment the LLM's context.
    *   **Semantic Search:** Instead of keyword matching, search for documents based on meaning.
    *   **Clustering and Classification:** Grouping similar texts or classifying them based on their semantic content.
    *   **Anomaly Detection:** Identifying text that is semantically distant from a known cluster.
*   **Tools:** Dedicated embedding models (e.g., OpenAI's `text-embedding-ada-002`, Sentence-BERT, various models on Hugging Face).

**Hands-On Exercise: Calculating Embedding Similarity**
1.  **Obtain Embeddings:** Use an embedding API (e.g., OpenAI's Embedding API) or a Python library (e.g., `sentence-transformers`) to generate embeddings for the following sentences:
    *   Sentence A: "Artificial intelligence is transforming industries."
    *   Sentence B: "AI is revolutionizing businesses."
    *   Sentence C: "The cat sat on the mat."
2.  **Compute Cosine Similarity:** Use the provided Python snippet (or a similar function in your chosen environment) to calculate the cosine similarity between:
    *   A and B
    *   A and C
    *   B and C

    ```python
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np

    # Assume 'embedding_a', 'embedding_b', 'embedding_c' are numpy arrays
    # representing the embeddings of Sentence A, B, and C respectively.
    # For example:
    # embedding_a = np.array([...])
    # embedding_b = np.array([...])
    # embedding_c = np.array([...])

    similarity_ab = cosine_similarity(embedding_a.reshape(1, -1), embedding_b.reshape(1, -1))[0][0]
    similarity_ac = cosine_similarity(embedding_a.reshape(1, -1), embedding_c.reshape(1, -1))[0][0]
    similarity_bc = cosine_similarity(embedding_b.reshape(1, -1), embedding_c.reshape(1, -1))[0][0]

    print(f"Similarity (A, B): {similarity_ab:.4f}")
    print(f"Similarity (A, C): {similarity_ac:.4f}")
    print(f"Similarity (B, C): {similarity_bc:.4f}")
    ```
3.  **Analyze Results:** Observe how the similarity scores reflect the semantic relatedness of the sentences. Sentences A and B should have a high similarity, while those involving C should have lower scores.

## 3. Vector Spaces: The Semantic Landscape

**Concept:** The collection of all possible embeddings forms a high-dimensional *vector space*. Each token, word, phrase, or document exists as a point within this space. The geometry of this space captures complex semantic relationships.

*   **Dimensionality:** These spaces typically have hundreds or thousands of dimensions (e.g., 768 for BERT-base, 1536 for OpenAI's `text-embedding-ada-002`).
*   **Semantic Relationships:** Analogies like "King - Man + Woman = Queen" demonstrate that vector operations can capture relational semantics. Moving in a certain direction in the vector space can correspond to a change in meaning (e.g., gender, tense, country).
*   **Visualization:** While impossible to visualize directly in high dimensions, techniques like Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) can project these vectors into 2D or 3D, allowing for visual inspection of clusters and relationships.
*   **Relevance to Prompting:** Understanding vector spaces helps in appreciating how LLMs "reason" about meaning and how providing relevant context (which translates to points in this space) guides the model towards desired outputs.

**Hands-On Exercise: Visualizing Embeddings (Conceptual)**
*Note: This exercise requires a Python environment with `numpy`, `scikit-learn`, and `matplotlib` or `plotly`.*
1.  **Generate Embeddings:** Obtain embeddings for a small, diverse set of sentences (e.g., 10-20 sentences about different topics like "dogs," "cats," "cars," "computers," "weather").
2.  **Dimensionality Reduction:** Apply t-SNE to reduce the embeddings to 2 dimensions.
    ```python
    from sklearn.manifold import TSNE
    import matplotlib.pyplot as plt
    import numpy as np

    # Assume 'all_embeddings' is a numpy array of shape (num_sentences, embedding_dim)
    # For example:
    # all_embeddings = np.array([...]) # Your collected embeddings

    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(all_embeddings)-1))
    reduced_embeddings = tsne.fit_transform(all_embeddings)

    # Plotting (conceptual - you'd need labels for each point)
    plt.figure(figsize=(8, 6))
    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])
    plt.title("2D t-SNE Projection of Sentence Embeddings")
    plt.xlabel("t-SNE Component 1")
    plt.ylabel("t-SNE Component 2")
    plt.show()
    ```
3.  **Observe Clusters:** If you label your points (e.g., color-code by topic), you should observe that sentences about similar topics tend to cluster together in the 2D plot, demonstrating the semantic organization of the vector space.

## Practical Implications for Prompt Engineering

*   **Context Window Management:** Be mindful of token limits. If your prompt and context exceed the limit, the LLM will truncate it, potentially losing critical information. Use tokenizers to estimate length.
*   **Semantic Search for RAG:** Leverage embeddings to retrieve relevant information from external knowledge bases. This allows you to provide highly specific and up-to-date context to your LLM, overcoming its inherent knowledge cutoff.
*   **Few-Shot Learning:** The effectiveness of few-shot examples (covered in Chapter 3) relies on the LLM's ability to understand the semantic relationship between the examples and the new query within its embedding space.
*   **Prompt Optimization:** Understanding that prompts are converted to vectors can inform how you phrase instructions. Semantically similar prompts will likely lead to similar internal representations, but subtle differences can shift the vector enough to alter output.

## Reflection

*   How does the concept of tokenization directly influence the practical constraints (like cost and length) you face when designing prompts for LLMs?
*   In what scenarios would calculating the cosine similarity of embeddings be a useful technique for a prompt engineer?
*   How does the idea of a "vector space" help you conceptualize how LLMs understand the meaning and relationships between words and sentences in your prompts?
*   Consider a situation where your prompt is too long. How would your understanding of tokenization guide your approach to shortening it without losing critical information?
