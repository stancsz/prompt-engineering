# 2.2 Pretraining vs. Fine-Tuning vs. Instruction Tuning

Large language models go through multiple stages of training to become useful:

## Pretraining

- **Objective:** Learn general language patterns from massive unlabeled text.
- **Data:** Web pages, books, articles.
- **Outcome:** A model that “knows” grammar, facts, and common sense.

## Fine-Tuning

- **Objective:** Adapt pretrained weights on a smaller, task-specific dataset.
- **Data:** Labeled examples (e.g., sentiment analysis, question-answer pairs).
- **Outcome:** Specialized performance on one task.

## Instruction Tuning

- **Objective:** Teach the model to follow instructions by fine-tuning on prompt-response pairs.
- **Data:** Human-written prompts with desired outputs.
- **Outcome:** Better zero-shot and few-shot behavior on new instructions.

## Example Prompts

- **Zero-Shot (Instruction Tuning):**  
  ```
  Translate this sentence to French: “Machine learning is fascinating.”
  ```
- **Fine-Tuned Model Query:**  
  ```
  Q: What is the sentiment of “I love prompt engineering”?  
  A:
  ```

## Hands-On Exercise

1. In an LLM playground, compare:
   - A raw pretrained model completing generic text.
   - A fine-tuned sentiment model on “I hate waiting in traffic.”
   - An instruction-tuned model translating or following other commands.  
2. Record differences in coherence and accuracy.

## Reflection

- How did fine-tuning change behavior compared to pretraining alone?  
- Did instruction-tuned outputs feel more compliant with your commands?
