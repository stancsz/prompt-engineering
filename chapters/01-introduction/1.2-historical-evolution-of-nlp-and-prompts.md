# 1.2 Historical Evolution of NLP and Prompts

Natural Language Processing (NLP) has progressed from rule-based systems to large neural language models. Prompting evolved alongside:

- **1970s–1990s**: Rule-based grammars, keyword spotting.  
- **2000s**: Statistical models (n-grams, Hidden Markov Models).  
- **2010s**: Neural sequence models (RNNs, LSTMs).  
- **2020+**: Transformer architectures; few-shot prompting in GPT-3.

## Example: From Rules to Prompts

1. **Rule-Based**  
   ```
   IF sentence CONTAINS "weather" THEN reply "The weather is sunny."
   ```
2. **Prompt-Based**  
   ```
   You are a weather bot. Answer: "What is today’s weather?" -> 
   ```

## Hands-On Exercise

1. Find an online demo of an early NLP chatbot (e.g., ELIZA).  
2. Compare its rigid responses to a modern LLM prompt:  
   ```
   You are a conversational assistant. How would you respond to "I feel anxious today"?
   ```  
3. Note differences in flexibility, context, and empathy.

## Reflection

- How did the transition to neural models change the role of prompt design?  
- What limitations did rule-based systems face that prompting overcomes?
