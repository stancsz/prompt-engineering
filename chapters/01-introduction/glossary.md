# Glossary of Terms & Acronyms - Chapter 01

**Prompt engineering**: the discipline of designing, refining, and optimizing the inputs (prompts) provided to large language models (LLMs) to achieve desired, accurate, and contextually relevant outputs.
**LLMs (Large Language Models)**: powerful, but their effectiveness is highly dependent on the quality of the input they receive.
**Clarity and Specificity**: Prompts should be unambiguous, detailing exactly what is expected from the model.
**Context Provision**: Supplying relevant background information helps the model generate more informed and accurate responses.
**Role Assignment**: Defining a persona for the LLM (e.g., "You are an expert historian") can significantly influence the style and content of its output.
**Constraint Definition**: Specifying limitations on length, format, or content helps narrow down the model's output space.
**Iterative Refinement**: Prompt engineering is an iterative process. Initial prompts are rarely perfect and require continuous testing and adjustment.
**Few-Shot Learning**: For complex tasks, include a few input-output examples to demonstrate the desired pattern.
**Delimiters**: Employ clear separators (e.g., triple quotes, XML tags) to distinguish instructions from context or examples.
**NLP (Natural Language Processing)**: has undergone a significant transformation, evolving from rudimentary rule-based systems to sophisticated large language models (LLMs).
**Rule-Based and Symbolic Systems**: NLP systems relied on handcrafted rules, lexicons, and grammatical structures.
**Statistical NLP**: Shifted to machine learning models trained on large text corpora.
**Deep Learning and Neural Networks**: Introduction of neural sequence models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTMs).
**Transformer Architecture**: (e.g., BERT, GPT) revolutionized NLP. Its self-attention mechanism allowed for parallel processing of sequences and superior capture of long-range dependencies.
**Instruction tuning**: where models were trained to follow instructions.
**ETL (Extract, Transform, Load)**: Data Extraction and Transformation.
