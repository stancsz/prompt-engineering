# 3.2 Instruction Prompts vs. Example-Based Prompts

When designing prompts for Large Language Models (LLMs), a fundamental decision involves choosing between two primary styles: **instruction-based prompts** and **example-based prompts**. Each style has distinct advantages and limitations, and the optimal choice depends on the complexity of the task, the desired output format, and the specific capabilities of the LLM being used. Often, a combination of both yields the best results.

## 1. Instruction-Based Prompts

**Definition:** An instruction-based prompt provides explicit, natural language commands or directives that describe the task the LLM should perform. The model relies solely on these instructions and its pre-trained knowledge to generate a response. This is essentially zero-shot learning (as discussed in Chapter 3.1).

*   **Strengths:**
    *   **Conciseness:** Typically shorter, consuming fewer tokens and potentially reducing API costs and latency.
    *   **Clear Intent:** Directly states the desired action, making the prompt's purpose unambiguous.
    *   **Flexibility:** Easy to modify instructions for different variations of a task.
    *   **Simplicity:** Ideal for straightforward tasks where the desired output format is intuitive or easily described.
*   **Limitations:**
    *   **Ambiguity:** Can be challenging for complex tasks where nuances of format, style, or specific interpretation are hard to convey purely through text.
    *   **Lack of Specificity:** The model might generate outputs that are technically correct but don't match a desired subtle style or tone.
    *   **Reliance on Model's Prior Knowledge:** Assumes the model has sufficient pre-trained knowledge to understand and execute the instruction without explicit demonstrations.

**Example:**
```
Summarize the following customer review in one concise sentence, focusing on the main complaint.

Review: "The delivery was incredibly slow, taking over a week, and when the package finally arrived, the item was damaged. I'm very disappointed with the service."
```
*Critique:* This prompt clearly instructs the model on the task (summarize), constraints (one sentence, main complaint), and input.

## 2. Example-Based Prompts (Few-Shot Learning)

**Definition:** An example-based prompt (also known as few-shot prompting) provides one or more input-output examples *before* the actual query. These examples demonstrate the desired task, format, style, or reasoning process, allowing the LLM to infer the pattern. This is a form of in-context learning.

*   **Strengths:**
    *   **Demonstrates Format and Style:** Highly effective for conveying complex output structures (e.g., JSON, tables, specific markdown) or nuanced stylistic requirements (e.g., formal, humorous, specific persona).
    *   **Improved Accuracy for Complex Tasks:** For tasks requiring specific reasoning steps or domain-specific knowledge, examples can significantly boost performance.
    *   **Reduces Ambiguity:** Shows the model *how* to perform the task rather than just telling it.
    *   **Handles Nuance:** Can convey subtle patterns that are difficult to articulate purely with instructions.
*   **Limitations:**
    *   **Token Consumption:** Examples add to the prompt length, potentially hitting context window limits and increasing API costs.
    *   **Example Quality:** The effectiveness heavily relies on the quality, diversity, and representativeness of the provided examples. Poor examples can mislead the model.
    *   **Scalability:** Manually creating many high-quality examples can be time-consuming.

**Example:**
```
Extract the product name and price from the following customer inquiries.

Inquiry: "I'd like to buy the new 'Quantum Leap' headphones for $249.99."
Output: {"product_name": "Quantum Leap headphones", "price": "$249.99"}

Inquiry: "How much is the 'Sonic Boom' speaker? I saw it for $120."
Output: {"product_name": "Sonic Boom speaker", "price": "$120"}

Inquiry: "Do you have the 'AeroGlide' drone? What's its price?"
Output:
```
*Critique:* The examples clearly demonstrate the desired JSON output format and the specific entities to extract, even when the phrasing varies.

## 3. Hybrid Approaches: Combining Instructions and Examples

For many real-world applications, the most effective prompts combine both explicit instructions and illustrative examples.

*   **Instruction + Few-Shot:** Provide clear instructions at the beginning of the prompt, then follow with a few examples to reinforce the desired behavior, format, or style. This leverages the strengths of both approaches.
    *   *Example:* "You are a professional copy editor. Correct grammar and spelling, and improve sentence flow. Here are some examples:" (followed by input/output pairs).

## When to Choose Which Approach

| Factor                | Instruction-Based Prompts                               | Example-Based Prompts (Few-Shot)                               |
| :-------------------- | :------------------------------------------------------ | :------------------------------------------------------------- |
| **Task Complexity**   | Simple, well-defined tasks                              | Complex, nuanced tasks; specific reasoning required             |
| **Output Format**     | Standard, easily described formats (e.g., plain text)   | Complex, structured formats (e.g., JSON, tables, specific markdown) |
| **Desired Style/Tone**| General tone (e.g., "polite," "formal")                 | Highly specific or subtle style/tone                           |
| **Context Window**    | Preferable for very long inputs or limited context windows | Consumes more tokens; may hit limits for long examples         |
| **Data Availability** | No examples needed                                      | Requires high-quality, representative examples                 |
| **Iteration Speed**   | Faster to iterate on instructions                       | Slower due to example creation and prompt length               |
| **Performance**       | Good for basic tasks                                    | Often higher accuracy and consistency for complex tasks        |

## Hands-On Exercise: Comparing Prompt Styles

1.  **Choose a Task:** Select a task that requires a specific output format, such as converting natural language requests into SQL queries (simplified).
2.  **Instruction-Based Attempt:**
    *   Prompt: `Convert the following natural language request into a SQL query to select all users from the 'users' table where their age is greater than 30.`
    *   Test with: `Find all users older than 30.`
    *   Observe the output. Does it generate valid SQL? Is the format consistent?
3.  **Example-Based Attempt (Few-Shot):**
    *   Create two examples for the same task:
        ```
        Convert the following natural language request into a SQL query.

        Request: "Show me all products with a price less than 50."
        SQL: SELECT * FROM products WHERE price < 50;

        Request: "List all orders placed in January 2023."
        SQL: SELECT * FROM orders WHERE order_date BETWEEN '2023-01-01' AND '2023-01-31';

        Request: "Find all users older than 30."
        SQL:
        ```
    *   Test with the same request: `Find all users older than 30.`
    *   Compare the output to the instruction-based attempt. Is the SQL more accurate or consistently formatted?

## Reflection

*   Which prompting style yielded more accurate or consistently formatted SQL queries? Why do you think this was the case?
*   When would the overhead of creating examples for an example-based prompt be justified by the improved output quality?
*   Can you envision a scenario where a purely instruction-based prompt would be superior, even for a somewhat complex task?
*   How might you combine instructions and examples to create an even more robust prompt for the SQL generation task?

## Best Practices for Both Styles

*   **Be Clear and Concise:** Regardless of style, avoid ambiguity.
*   **Use Delimiters:** For example-based prompts, use clear separators (e.g., `---`, `###`, XML tags) to distinguish instructions, examples, and the actual query.
*   **Iterate:** Prompt engineering is an iterative process. Start simple and add complexity as needed.
*   **Test Thoroughly:** Evaluate prompts with diverse inputs to ensure robustness.
*   **Consider Model Capabilities:** Some models are better at instruction following, while others benefit more from examples.
