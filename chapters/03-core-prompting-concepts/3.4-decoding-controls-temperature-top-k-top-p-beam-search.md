# 3.4 Decoding Controls: Temperature, Top-k, Top-p, Beam Search

Decoding controls govern how the model selects tokens during generation, trading off creativity, diversity, and determinism.

## Temperature

- **Definition:** Softens or sharpens the probability distribution.  
- **Range:** 0.0 (deterministic) to >1.0 (more random).  
- **Use Case:** Lower for precise answers; higher for creative text.

## Top-k Sampling

- **Definition:** Restricts choices to the k most probable tokens.  
- **Effect:** Avoids rare, low-probability tokens; balances quality vs. diversity.

## Top-p (Nucleus) Sampling

- **Definition:** Chooses from the smallest set of tokens whose cumulative probability ≥ p.  
- **Effect:** Dynamically adapts cutoff for diverse yet coherent outputs.

## Beam Search

- **Definition:** Keeps the top N most likely “beams” (partial sequences) at each step.  
- **Effect:** More exhaustive search for high-probability outputs; deterministic if beams=1.

## Examples

```
# Temperature
curl https://api.openai.com/v1/completions \
  -d '{"model":"gpt-4","prompt":"Tell a funny joke.","temperature":0.7}'

# Top-k
"top_k": 50

# Top-p
"top_p": 0.9

# Beam Search (pseudo)
beams = 5
```

## Hands-On Exercise

1. In your playground or via API:
   - Generate a sentence with temperature = 0.2 vs. 1.0.  
   - Generate with top_k=5 vs. top_k=50.  
   - Generate with top_p=0.5 vs. top_p=0.95.  
2. Compare diversity, coherence, and repetition.
3. If available, run beam search with beam size 3 and contrast with greedy decoding.

## Reflection

- Which setting gave the most creative output?  
- How did reducing top_k or top_p affect coherence?  
- When would you choose beam search over sampling?
