# 4.3 Self-Consistency, Voting, and Ensemble Prompts

Ensemble methods improve reliability by aggregating multiple independent generations.

## Self-Consistency

- **Definition:** Sample multiple chains of thought or outputs, then choose the most frequent answer.  
- **Benefit:** Reduces noise from a single random sample.

### Example

```text
Prompt: Solve “17 × 6” with Chain-of-Thought.

– Run 5 times with temperature=0.7.
– Collect each final answer.
– Output the answer that appears most often.
```

## Voting

- **Definition:** Generate several outputs from the same prompt and “vote” on the best or most common.  
- **Use Case:** Classification tasks (e.g., sentiment, topic).

### Example

```
Prompt: “Is this review positive or negative? ‘The product broke after two uses.’”

– Generate 7 completions.
– Count “Negative” vs. “Positive” votes.
– Return majority label.
```

## Ensemble Prompts

- **Definition:** Use different prompt templates or model variants and aggregate results.  
- **Benefit:** Leverages complementary strengths of prompts and models.

### Example

```
1. Template A: Role-based instruction.
2. Template B: Example-based prompt.
3. Template C: Open-ended instruction.

– Generate answer from each.
– Select consensus or highest-confidence result.
```

## Hands-On Exercise

1. Pick a simple math or classification prompt.  
2. Run 5–10 completions with varied temperatures.  
3. Tabulate outputs, apply majority vote, and compare to single-run accuracy.

## Reflection

- Did ensemble methods improve correctness?  
- How many samples were needed for stable consensus?
